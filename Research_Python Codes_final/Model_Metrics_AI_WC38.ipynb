{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dIWG8JChEONy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6447d754-39c4-488e-dd42-785653bce4cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.metrics import specificity_score\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9cRWgv4NE0b7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "def compute_classification_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute a suite of common binary/multiclass classification metrics\n",
        "    and return them in a single‐row DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array‐like of shape (n_samples,)\n",
        "        Ground‐truth labels (0/1 or multiclass).\n",
        "    y_pred : array‐like of shape (n_samples,)\n",
        "        Predicted labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        One‐row DataFrame with index [\"Metrics\"] and columns:\n",
        "        Accuracy, Recall, Specificity, Precision, F1, AUC.\n",
        "    \"\"\"\n",
        "    # Core scores\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    # specificity via recall on the “negative” class\n",
        "    spec = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "    f1   = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    # AUC for binary only (y_pred should be probabilities if you want the best AUC).\n",
        "    # Here we’ll use the hard preds, but ideally swap y_pred→y_score (proba) for real AUC.\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_pred)\n",
        "    except ValueError:\n",
        "        # if multiclass or only one class present, set to NaN\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    # Assemble into a DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        \"Accuracy\":    [acc],\n",
        "        \"Recall\":      [rec],\n",
        "        \"Specificity\": [spec],\n",
        "        \"Precision\":   [prec],\n",
        "        \"F1\":          [f1],\n",
        "        \"AUC\":         [auc],\n",
        "    }, index=[\"Metrics\"])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "7bBlm3-dPLED"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INPUT_EXCEL_FILE1=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_openai_simp_o4_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE2=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_deepseek_simp_deepseek_chat.xlsx\"\n",
        "INPUT_EXCEL_FILE3=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_anthropic_simp_claude_3.0_haiku.xlsx\"\n",
        "INPUT_EXCEL_FILE4=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_gemini_simp_both.xlsx\"\n",
        "INPUT_EXCEL_FILE5=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_gemini_simp_gemini_1.5_flash_002.xlsx\"\n",
        "INPUT_EXCEL_FILE6=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_openai_simp_gpt-3.5-turbo-0125.xlsx\"\n",
        "INPUT_EXCEL_FILE7=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_openai_simp_gpt_4.1_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE8=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Issues/research_ready_issues_openai_simp_o4_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE9=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_deepseek_simp_deepseek_chat.xlsx\"\n",
        "INPUT_EXCEL_FILE10=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_anthropic_simp_claude_3.0_haiku.xlsx\"\n",
        "INPUT_EXCEL_FILE11=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_gemini_simp_gemini-1.5-pro.xlsx\"\n",
        "INPUT_EXCEL_FILE12=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_gemini_simp_gemini_1.5_flash_002.xlsx\"\n",
        "INPUT_EXCEL_FILE13=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_gemini_simp_gemini_2.0_flash.xlsx\"\n",
        "INPUT_EXCEL_FILE14=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_openai_simp_gpt_3.5_turbo.xlsx\"\n",
        "INPUT_EXCEL_FILE15=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_openai_simp_gpt_4.1_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE16=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_openai_simp_o4_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE17=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/COT_Issues/research_ready_issues_anthropic_COT_claude_3.0_haiku.xlsx\"\n",
        "INPUT_EXCEL_FILE18=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/COT_Issues/research_ready_issues_gemini_COT_gemini_1.5_flash_002.xlsx\"\n",
        "INPUT_EXCEL_FILE19=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/COT_Issues/research_ready_issues_openai_COT_gpt_4.1_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE20=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/COT_facts/research_ready_facts_gemini_COT_gemini_1.5_flash_002.xlsx\"\n",
        "INPUT_EXCEL_FILE21=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/COT_facts/research_ready_facts_openai_COT_gpt_4.1_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE22=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/COT_facts/research_ready_facts_anthropic_COT_claude_3.0_haiku.xlsx\"\n",
        "INPUT_EXCEL_FILE23=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Sampling/research_ready_issues_AI_20_sampled_gpt_4.1_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE24=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Sampling/research_ready_issues_AI_20_sampled_gemini_1.5_flash_002.xlsx\"\n",
        "INPUT_EXCEL_FILE25=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Sampling/research_ready_facts_AI_20_sampled_gpt_4.1_mini.xlsx\"\n",
        "INPUT_EXCEL_FILE26=\"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Sampling/research_ready_facts_AI_20_sampled_gemini_1.5_flash_002.xlsx\""
      ],
      "metadata": {
        "id": "lfKR_MduEun4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e4NOoyh7jXo"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "df = pd.read_excel(INPUT_EXCEL_FILE11)  # Make sure the file is in the same directory or provide full path\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRUE_COL_NAME=\"Decision\"\n",
        "PRED_COL_NAME=\"claude_3_haiku_20240307_AI_Decision1\"\n",
        "\n",
        "# Extract true labels and predictions\n",
        "y_true = df[TRUE_COL_NAME]\n",
        "y_pred = df[PRED_COL_NAME]\n",
        "\n",
        "print(y_true.unique())\n",
        "print(y_pred.unique())\n",
        "\n",
        "#exclude any rows where y_pred=-1 from df\n",
        "df = df[df[PRED_COL_NAME] != -1]\n",
        "\n",
        "# Extract true labels and predictions\n",
        "y_true = df[TRUE_COL_NAME]\n",
        "y_pred = df[PRED_COL_NAME]\n",
        "\n",
        "df_metrics = compute_classification_metrics(y_true, y_pred)\n",
        "print(df_metrics.round(4))"
      ],
      "metadata": {
        "id": "H0vjmvEMd7Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "\n",
        "def compute_classification_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute a suite of common binary/multiclass classification metrics\n",
        "    and return them in a single-row DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like of shape (n_samples,)\n",
        "        Ground-truth labels (0/1 or multiclass).\n",
        "    y_pred : array-like of shape (n_samples,)\n",
        "        Predicted labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        One-row DataFrame with index [\"Metrics\"] and columns:\n",
        "        Accuracy, Recall, Specificity, Precision, F1, AUC.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine if it's binary or multiclass\n",
        "    is_binary = len(np.unique(y_true)) == 2\n",
        "\n",
        "    # Core scores\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1   = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    # Specificity for binary classification (recall of the negative class)\n",
        "    # For multiclass, calculating a single 'specificity' is more complex and less standard,\n",
        "    # often requiring a one-vs-rest approach. Here, we'll keep it simple for binary.\n",
        "    if is_binary:\n",
        "        # Assuming 0 is the negative class\n",
        "        spec = recall_score(y_true, y_pred, pos_label=0, average='binary', zero_division=0)\n",
        "    else:\n",
        "        # For multiclass, set specificity to NaN or handle as appropriate for your needs.\n",
        "        # A common approach is to compute it per class and average, but 'specificity'\n",
        "        # isn't as directly applicable across all multiclass scenarios.\n",
        "        spec = float(\"nan\") # Or implement one-vs-rest specificity if needed\n",
        "\n",
        "    # AUC for binary only (y_pred should be probabilities if you want the best AUC).\n",
        "    # Here we’ll use the hard preds. If you have probabilities (y_score), use y_score instead of y_pred.\n",
        "    auc = float(\"nan\") # Initialize AUC to NaN\n",
        "    if is_binary:\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, y_pred)\n",
        "        except ValueError:\n",
        "            # This can happen if only one class is present in y_true_clean or y_pred_clean\n",
        "            auc = float(\"nan\")\n",
        "    else:\n",
        "        # For multiclass AUC, you'd typically use 'ovr' (one-vs-rest) or 'ovo' (one-vs-one) strategies,\n",
        "        # and roc_auc_score generally expects probability estimates (y_score) for multiclass.\n",
        "        # If y_pred are hard labels, this will likely fail or give misleading results.\n",
        "        # If you have probabilities, you'd need to binarize y_true for multiclass AUC.\n",
        "        # Example for multiclass AUC with probabilities (assuming y_pred were y_score):\n",
        "        # lb = LabelBinarizer()\n",
        "        # lb.fit(y_true)\n",
        "        # y_true_binarized = lb.transform(y_true)\n",
        "        # y_pred_binarized = lb.transform(y_pred) # This would be y_score_binarized if you had probas\n",
        "        # auc = roc_auc_score(y_true_binarized, y_pred_binarized, average='macro')\n",
        "        pass\n",
        "\n",
        "\n",
        "    # Assemble into a DataFrame\n",
        "    df_metrics = pd.DataFrame({\n",
        "        \"Accuracy\":      [acc],\n",
        "        \"Recall\":        [rec],\n",
        "        \"Specificity\":   [spec],\n",
        "        \"Precision\":     [prec],\n",
        "        \"F1\":            [f1],\n",
        "        \"AUC\":           [auc],\n",
        "    }, index=[\"Metrics\"])\n",
        "\n",
        "    return df_metrics\n",
        "\n",
        "\n",
        "df = pd.read_excel(INPUT_EXCEL_FILE26)  # Make sure the file is in the same directory or provide full path\n",
        "# --- End of Placeholder ---\n",
        "\n",
        "\n",
        "TRUE_COL_NAME = df.columns[2] # Assuming your true labels are in the first column for this example\n",
        "print(f\"True labels column: {TRUE_COL_NAME!r}\\n\")\n",
        "\n",
        "records = []\n",
        "\n",
        "# Adjust column slicing based on your actual df structure.\n",
        "# For the placeholder, columns 1 to 21 (index 1 to 20) contain model predictions.\n",
        "# Make sure your TRUE_COL_NAME is not included in this range.\n",
        "# For the provided sample data, `df.columns[1:]` would work for all models.\n",
        "model_prediction_cols = df.columns[3:len(df.columns)] # Adjust this range if your actual df differs\n",
        "\n",
        "for col in model_prediction_cols:\n",
        "    # print(f\"=== Results for prediction column: {col!r} ===\") # Uncomment for detailed per-model print\n",
        "\n",
        "    # Extract and clean\n",
        "    y_true = df[TRUE_COL_NAME]\n",
        "    y_pred = df[col]\n",
        "\n",
        "    # Exclude rows where prediction == -1\n",
        "    mask = (y_pred != -1)\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    # Ensure there are samples after cleaning\n",
        "    if len(y_true_clean) == 0:\n",
        "        print(f\"Warning: No valid samples for model {col!r} after excluding -1. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_classification_metrics(y_true_clean, y_pred_clean)\n",
        "    # The compute_classification_metrics returns a DataFrame with index 'Metrics'.\n",
        "    # We want to use the column name as the model name.\n",
        "    # We'll transform this single-row DataFrame into a Series or a dictionary to append.\n",
        "    model_metrics_series = metrics.iloc[0].rename(col) # Rename the index to the model name\n",
        "    records.append(model_metrics_series)\n",
        "\n",
        "# 3) Build the DataFrame\n",
        "# Use pd.concat to combine Series into a DataFrame, then transpose.\n",
        "metrics_df = pd.DataFrame(records)\n",
        "\n",
        "# 4) Ensure we have a “Model” column to index on\n",
        "# The current approach already sets model names as column names,\n",
        "# then transposes them to be the index. So 'Model' column is not needed as a separate column.\n",
        "# The index of metrics_df will automatically be the model names.\n",
        "\n",
        "# 5) Round and display per-model metrics\n",
        "print(\"Per-model metrics:\")\n",
        "print(metrics_df.round(4))\n",
        "\n",
        "# 6) Compute overall mean and std\n",
        "summary = pd.DataFrame({\n",
        "    'mean': metrics_df.mean(),\n",
        "    'std':  metrics_df.std()\n",
        "})\n",
        "\n",
        "print(\"\\nOverall summary (mean ± std):\")\n",
        "for metric in summary.index:\n",
        "    m = summary.loc[metric, 'mean']\n",
        "    s = summary.loc[metric, 'std']\n",
        "    print(f\"  {metric:12s}: {m:.4f} ± {s:.4f}\")"
      ],
      "metadata": {
        "id": "2G17buOhVgUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of file paths\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_deepseek_simp_deepseek_chat.xlsx\"\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_anthropic_simp_claude_3.0_haiku.xlsx\",\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_gemini_simp_gemini-1.5-pro.xlsx\",\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_gemini_simp_gemini_1.5_flash_002.xlsx\",\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_gemini_simp_gemini_2.0_flash.xlsx\",\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_openai_simp_gpt_3.5_turbo.xlsx\",\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_openai_simp_gpt_4.1_mini.xlsx\",\n",
        "    \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_openai_simp_o4_mini.xlsx\"\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Loop through each file path\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        # Read the Excel file\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        # Select the first column (index 0) and the fourth column (index 3)\n",
        "        # Get filename without path and extension to use as column name\n",
        "        filename = file_path.split('/')[-1].split('.')[0]\n",
        "        selected_df = df.iloc[:, [0, 3]].copy()\n",
        "        selected_df.columns = ['ID', filename]  # Rename columns\n",
        "\n",
        "        # Append to the list of dataframes\n",
        "        dfs.append(selected_df)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}. Please check the path and try again.\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Check if any dataframes were successfully loaded\n",
        "if not dfs:\n",
        "    print(\"No dataframes were loaded. Cannot merge.\")\n",
        "else:\n",
        "    # Merge all dataframes on the 'ID' column\n",
        "    # Start with the first dataframe and merge sequentially\n",
        "    merged_df = dfs[0]\n",
        "    for i in range(1, len(dfs)):\n",
        "        merged_df = pd.merge(merged_df, dfs[i], on='ID', how='outer') # Using outer join to keep all IDs\n",
        "\n",
        "    # Identify columns containing the 0s and 1s (all columns except 'ID')\n",
        "    data_columns = [col for col in merged_df.columns if col != 'ID']\n",
        "\n",
        "    # Calculate the sum of values (0s and 1s) for each row across the data columns\n",
        "    # If a value is NaN, it will be treated as 0 for the sum, which is appropriate for majority decision.\n",
        "    merged_df['AI_Majority_Decision'] = merged_df[data_columns].sum(axis=1)\n",
        "\n",
        "    # Determine the majority decision:\n",
        "    # If the sum of 1s is greater than half the number of data columns, the majority is 1.\n",
        "    # Otherwise (sum is less than or equal to half), the majority is 0.\n",
        "    # This handles ties by favoring 0.\n",
        "    num_data_columns = len(data_columns)\n",
        "    if num_data_columns > 0:\n",
        "        merged_df['AI_Majority_Decision'] = (merged_df['AI_Majority_Decision'] > (num_data_columns / 2)).astype(int)\n",
        "    else:\n",
        "        # If there are no data columns, the majority decision cannot be determined.\n",
        "        merged_df['AI_Majority_Decision'] = None\n",
        "\n",
        "    # --- Start of new code to combine \"Decision\" column ---\n",
        "    decision_file_path = \"/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_anthropic_simp_claude_3.0_haiku.xlsx\"\n",
        "    try:\n",
        "        decision_df = pd.read_excel(decision_file_path)\n",
        "        # Select the first column (ID) and the third column (Decision)\n",
        "        decision_df_selected = decision_df.iloc[:, [0, 2]].copy()\n",
        "        decision_df_selected.columns = ['ID', 'Decision'] # Rename columns for clarity\n",
        "\n",
        "        # Merge the 'Decision' column into the main merged_df\n",
        "        merged_df = pd.merge(merged_df, decision_df_selected, on='ID', how='left') # Use left join to keep all IDs from merged_df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Decision file not found at {decision_file_path}. The 'Decision' column will not be added.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the decision file: {e}\")\n",
        "    # --- End of new code ---\n",
        "\n",
        "    # Display the merged dataframe\n",
        "    print(merged_df)\n"
      ],
      "metadata": {
        "id": "hg-nNAlrq6RS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e0537f-3262-44fe-b08a-7734de8014cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while processing /content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_deepseek_simp_deepseek_chat.xlsx/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_anthropic_simp_claude_3.0_haiku.xlsx: [Errno 20] Not a directory: '/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_deepseek_simp_deepseek_chat.xlsx/content/drive/MyDrive/worker_comp_work/WC_Final_Sheets_Categorized/Simple_prompt_Facts/research_ready_facts_anthropic_simp_claude_3.0_haiku.xlsx'\n",
            "             ID  research_ready_facts_gemini_simp_gemini-1  \\\n",
            "0        000301                                          1   \n",
            "1        000336                                          1   \n",
            "2        000344                                          1   \n",
            "3        000388                                          1   \n",
            "4        000406                                          1   \n",
            "...         ...                                        ...   \n",
            "14220    y31451                                          0   \n",
            "14221  y31451o2                                          1   \n",
            "14222    y31560                                          0   \n",
            "14223    y31853                                          1   \n",
            "14224  y31853o2                                          1   \n",
            "\n",
            "       research_ready_facts_gemini_simp_gemini_1  \\\n",
            "0                                              1   \n",
            "1                                              1   \n",
            "2                                              1   \n",
            "3                                              1   \n",
            "4                                              1   \n",
            "...                                          ...   \n",
            "14220                                          1   \n",
            "14221                                          1   \n",
            "14222                                          0   \n",
            "14223                                          1   \n",
            "14224                                          1   \n",
            "\n",
            "       research_ready_facts_gemini_simp_gemini_2  \\\n",
            "0                                              1   \n",
            "1                                              1   \n",
            "2                                              1   \n",
            "3                                              1   \n",
            "4                                              1   \n",
            "...                                          ...   \n",
            "14220                                          0   \n",
            "14221                                          1   \n",
            "14222                                          0   \n",
            "14223                                          1   \n",
            "14224                                          1   \n",
            "\n",
            "       research_ready_facts_openai_simp_gpt_3  \\\n",
            "0                                           1   \n",
            "1                                           1   \n",
            "2                                           1   \n",
            "3                                           1   \n",
            "4                                           1   \n",
            "...                                       ...   \n",
            "14220                                       1   \n",
            "14221                                       0   \n",
            "14222                                       0   \n",
            "14223                                       1   \n",
            "14224                                       1   \n",
            "\n",
            "       research_ready_facts_openai_simp_gpt_4  \\\n",
            "0                                           1   \n",
            "1                                           1   \n",
            "2                                           1   \n",
            "3                                           1   \n",
            "4                                           1   \n",
            "...                                       ...   \n",
            "14220                                       0   \n",
            "14221                                       1   \n",
            "14222                                       0   \n",
            "14223                                       1   \n",
            "14224                                       1   \n",
            "\n",
            "       research_ready_facts_openai_simp_o4_mini  AI_Majority_Decision  \\\n",
            "0                                             1                     1   \n",
            "1                                             1                     1   \n",
            "2                                             1                     1   \n",
            "3                                             1                     1   \n",
            "4                                             1                     1   \n",
            "...                                         ...                   ...   \n",
            "14220                                         1                     0   \n",
            "14221                                         1                     1   \n",
            "14222                                         0                     0   \n",
            "14223                                         1                     1   \n",
            "14224                                         1                     1   \n",
            "\n",
            "       Decision  \n",
            "0             1  \n",
            "1             1  \n",
            "2             1  \n",
            "3             1  \n",
            "4             1  \n",
            "...         ...  \n",
            "14220         1  \n",
            "14221         1  \n",
            "14222         0  \n",
            "14223         1  \n",
            "14224         1  \n",
            "\n",
            "[14225 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRUE_COL_NAME=\"AI_Majority_Decision\"\n",
        "PRED_COL_NAME=\"Decision\"\n",
        "\n",
        "# Extract true labels and predictions\n",
        "y_true = merged_df[TRUE_COL_NAME]\n",
        "y_pred = merged_df[PRED_COL_NAME]\n",
        "\n",
        "print(y_true.unique())\n",
        "print(y_pred.unique())\n",
        "\n",
        "#exclude any rows where y_pred=-1 from df\n",
        "merged_df = merged_df[merged_df[PRED_COL_NAME] != -1]\n",
        "\n",
        "# Extract true labels and predictions\n",
        "y_true = merged_df[TRUE_COL_NAME]\n",
        "y_pred = merged_df[PRED_COL_NAME]\n",
        "\n",
        "df_metrics = compute_classification_metrics(y_true, y_pred)\n",
        "print(df_metrics.round(4))\n"
      ],
      "metadata": {
        "id": "Ng_AL1aJroOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70a4bf6-99a4-4b79-b248-0be7040382b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n",
            "[1 0]\n",
            "         Accuracy  Recall  Specificity  Precision      F1     AUC\n",
            "Metrics    0.8577  0.8479       0.7326     0.8771  0.8525  0.8479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mismatched_rows_count = (merged_df['AI_Majority_Decision'] != merged_df['Decision']).sum()\n",
        "print(mismatched_rows_count)"
      ],
      "metadata": {
        "id": "Hlpmh3rRsU0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a7a6e1-57a0-4837-9d42-83873d317d70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.shape[0]\n"
      ],
      "metadata": {
        "id": "jouMpLFcukxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37ae426-3346-4ef2-ffbb-5f06e4e383b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14225"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(100*mismatched_rows_count/merged_df.shape[0])"
      ],
      "metadata": {
        "id": "yhH_GNEWuxDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef8327b-5c5f-454f-e7fd-e3811cd7bdd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.22847100175747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# This code block assumes 'comparable_df' is already defined and populated\n",
        "# with 'ID', 'AI_Majority_Decision', and 'Decision' columns,\n",
        "# and 'decision_file_path' is defined.\n",
        "\n",
        "# Get and print about 10 IDs where AI_Majority_Decision is not the same as Decision\n",
        "mismatched_ids = merged_df.loc[merged_df['AI_Majority_Decision'] != merged_df['Decision'], 'ID'].head(10).tolist()\n",
        "if mismatched_ids:\n",
        "    print(f\"IDs where AI_Majority_Decision is different from Decision (first {len(mismatched_ids)}): {mismatched_ids}\")\n",
        "else:\n",
        "    print(\"No IDs found where AI_Majority_Decision is different from Decision.\")\n",
        "\n",
        "# --- New code to print the second column for 5 mismatched IDs ---\n",
        "if mismatched_ids:\n",
        "    # Take only the first 5 mismatched IDs\n",
        "    ids_for_second_column = mismatched_ids[:5]\n",
        "    print(f\"\\nRetrieving second column for the first {len(ids_for_second_column)} mismatched IDs:\")\n",
        "    try:\n",
        "        # Read the original file again to get the second column\n",
        "        original_df = pd.read_excel(\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready.xlsx\")\n",
        "        # Filter by the selected mismatched IDs\n",
        "        filtered_original_df = original_df[original_df.iloc[:, 0].isin(ids_for_second_column)]\n",
        "        # Print the ID (first column) and the second column\n",
        "        if not filtered_original_df.empty:\n",
        "            for index, row in filtered_original_df.iterrows():\n",
        "                print(f\"ID: {row.iloc[0]}, Second Column: {row.iloc[3]}\")\n",
        "        else:\n",
        "            print(\"No matching IDs found in the original file for retrieving the second column.\")\n",
        "    except FileNotFoundError:\n",
        "        # Note: decision_file_path is used here as a placeholder for the original file path\n",
        "        # from the context of the main Canvas, assuming it refers to the same file.\n",
        "        print(f\"Error: Original file for second column retrieval not found at {decision_file_path}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while retrieving the second column: {e}\")\n",
        "# --- End of new code ---\n"
      ],
      "metadata": {
        "id": "xXiD8ehbxRB9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}