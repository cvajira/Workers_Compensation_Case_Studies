{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3skPK9heVGJU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sxUxWTLgVWsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_EXCEL_FILE1 =\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_facts_AI_sampled.xlsx\"\n",
        "OUTPUT_EXCEL_FILE1 =\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_facts_AI_sampled_gemini_1.5_flash_002.xlsx\""
      ],
      "metadata": {
        "id": "96GxltrMVcig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai google-api-core pandas==2.2.2"
      ],
      "metadata": {
        "id": "ZlLw5VWOVs1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import time\n",
        "from google.api_core import exceptions # Import for specific exceptions\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "LYqW92WZVtq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "iFWB-wW5VwCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "_n2U9etnmIzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=api_key)\n",
        "# 2. Specify the Gemini Model\n",
        "# Use the latest appropriate 'flash' model identifier (e.g., gemini-1.5-flash-latest)\n",
        "# MODEL_NAME_LIST = [\"gemini-1.5-pro\", \"gemini-2.0-flash\", gemini-1.5-flash-002]\n",
        "MODEL_NAME_LIST = [\"gemini-1.5-flash-002\"] # Or \"gemini-1.5-flash-latest\"\n",
        "\n",
        "# 3. Excel File Paths\n",
        "INPUT_EXCEL_FILE = INPUT_EXCEL_FILE1  # Replace with your input file name\n",
        "OUTPUT_EXCEL_FILE = OUTPUT_EXCEL_FILE1 # Changed output name slightly\n",
        "\n",
        "# 4. Column Names (Adjust if different in your Excel)\n",
        "# 4. Column Names (Adjust if different in your Excel)\n",
        "FACTS_COLUMN = \"Annonymized_Facts\"\n",
        "\n",
        "# 5. Time Delays (in seconds)\n",
        "MAIN_LOOP_DELAY_SECONDS = 0\n",
        "API_CALL_DELAY_SECONDS = 0  # Adjust as needed, especially if hitting rate limits\n",
        "\n",
        "# --- Create Gemini Model Instance (ONCE) ---\n",
        "# For structured JSON output, you MUST set response_mime_type to \"application/json\"\n",
        "generation_config = {\n",
        "    \"temperature\": 0, # Keep at 0 for deterministic extraction\n",
        "    \"top_p\": None,\n",
        "    \"top_k\": 1,\n",
        "    \"response_mime_type\": \"application/json\", # Crucial for JSON output\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "]\n",
        "\n",
        "# --- Function to get predictions from Gemini ---\n",
        "def get_gemini_predictions(facts_text, MODEL_NAME):\n",
        "    \"\"\"\n",
        "    Sends anonymized facts to Google Gemini and asks it to predict Case ID, Year,\n",
        "    Plaintiff Name, and Defendant Name.\n",
        "\n",
        "    Args:\n",
        "        facts_text (str): The text from the 'Annonymized_Facts' column.\n",
        "        MODEL_NAME (str): The Google Gemini model to use for the prediction.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with predicted 'Case ID', 'Year', 'Plaintiff Name',\n",
        "              'Defendant Name'. Returns None or default values if an error occurred.\n",
        "    \"\"\"\n",
        "    if not facts_text or not isinstance(facts_text, str) or len(facts_text.strip()) == 0:\n",
        "        print(\"Warning: Empty or invalid facts text provided. Returning default predictions.\")\n",
        "        return {\n",
        "            \"Predicted_ID\": \"N/A\",\n",
        "            \"Predicted_Year\": \"N/A\",\n",
        "            \"Predicted_Plaintiff\": \"N/A\",\n",
        "            \"Predicted_Defendent\": \"N/A\"\n",
        "        }\n",
        "\n",
        "    # New prompt to extract specific entities\n",
        "    # For Gemini, it's often better to put the JSON instruction clearly in the system/user role.\n",
        "    system_prompt = \"\"\"\n",
        "    You are an expert at extracting case information from legal summaries. Based solely on your internal training data and knowledge (no web search), identify the following for each anonymized workers' compensation case:\n",
        "    \"Case_ID\": (string, predicted Case ID, or \"Unknown\" if not found)\n",
        "    \"Year\": (string, predicted Year the case was heard, or \"Unknown\" if not found)\n",
        "    \"Plaintiff_Name\": (string, predicted Plaintiff's Name, or \"Unknown\" if not found)\n",
        "    \"Defendant_Name\": (string, predicted Defendant's Name, or \"Unknown\" if not found)\n",
        "\n",
        "    If a piece of information is explicitly stated as anonymized or cannot be confidently extracted, use \"Unknown\" for that specific key.\n",
        "    Your response MUST be a JSON object and contain ONLY the JSON object. Do NOT include any other text or explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    You are given the following anonymized workers compensation case. Now predict Case ID, Year of this Case heard, Plaintiff Name and Defendant Name.\n",
        "\n",
        "    Anonymized Case Facts:\n",
        "    ---\n",
        "    {facts_text}\n",
        "    ---\n",
        "    \"\"\"\n",
        "\n",
        "    # For Gemini, the prompt structure for JSON output is usually handled by the content.\n",
        "    # While we can't directly specify `response_format={\"type\": \"json_object\"}` like OpenAI,\n",
        "    # a strong system/user prompt usually guides Gemini to produce JSON.\n",
        "\n",
        "    predicted_values = {\n",
        "        \"Predicted_ID\": \"Error\",\n",
        "        \"Predicted_Year\": \"Error\",\n",
        "        \"Predicted_Plaintiff\": \"Error\",\n",
        "        \"Predicted_Defendent\": \"Error\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"Waiting for {API_CALL_DELAY_SECONDS} second(s) before API call...\")\n",
        "        time.sleep(API_CALL_DELAY_SECONDS)\n",
        "\n",
        "        print(f\"Making API call to {MODEL_NAME} for extraction...\")\n",
        "        # Use genai.GenerativeModel for chat interactions\n",
        "        model = genai.GenerativeModel(MODEL_NAME)\n",
        "        response = model.generate_content(\n",
        "            [{\"role\": \"user\", \"parts\": [system_prompt, user_prompt]}],\n",
        "            generation_config=generation_config,\n",
        "            safety_settings=safety_settings\n",
        "        )\n",
        "        print(\"API call complete.\")\n",
        "\n",
        "        # Access content based on Gemini's structure\n",
        "        response_content = response.text.strip()\n",
        "\n",
        "        # Attempt to parse the JSON response\n",
        "        try:\n",
        "            parsed_json = json.loads(response_content)\n",
        "            predicted_values[\"Predicted_ID\"] = parsed_json.get(\"Case_ID\", \"Unknown\")\n",
        "            predicted_values[\"Predicted_Year\"] = parsed_json.get(\"Year\", \"Unknown\")\n",
        "            predicted_values[\"Predicted_Plaintiff\"] = parsed_json.get(\"Plaintiff_Name\", \"Unknown\")\n",
        "            predicted_values[\"Predicted_Defendent\"] = parsed_json.get(\"Defendant_Name\", \"Unknown\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Warning: Could not parse JSON from response: '{response_content}' for facts: '{facts_text[:100]}...'\")\n",
        "            predicted_values[\"Predicted_ID\"] = \"Parsing Error\"\n",
        "            predicted_values[\"Predicted_Year\"] = \"Parsing Error\"\n",
        "            predicted_values[\"Predicted_Plaintiff\"] = \"Parsing Error\"\n",
        "            predicted_values[\"Predicted_Defendent\"] = \"Parsing Error\"\n",
        "\n",
        "    except Exception as e: # Catch broader exceptions for Gemini as it's not as granular as OpenAI's\n",
        "        print(f\"An unexpected error occurred during Google Gemini API call for facts: '{facts_text[:100]}...'. Error: {e}\")\n",
        "        # Consider specific error handling if google.api_core.exceptions is imported and used\n",
        "        # for more granular error types (e.g., RateLimitExceeded, Aborted).\n",
        "\n",
        "    return predicted_values\n",
        "\n",
        "# --- Main Processing Logic ---\n",
        "\n",
        "print(f\"\\nReading Excel file: {INPUT_EXCEL_FILE}\")\n",
        "try:\n",
        "    df = pd.read_excel(INPUT_EXCEL_FILE)\n",
        "    print(f\"Successfully read {len(df)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file not found at {INPUT_EXCEL_FILE}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading Excel file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Check for input column\n",
        "if FACTS_COLUMN not in df.columns:\n",
        "    print(f\"Error: Column '{FACTS_COLUMN}' not found in the Excel file.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize new prediction columns\n",
        "output_columns = [\"Predicted_ID\", \"Predicted_Year\", \"Predicted_Plaintiff\", \"Predicted_Defendent\"]\n",
        "for col in output_columns:\n",
        "    if col not in df.columns:\n",
        "        df[col] = \"Not Processed\" # Initialize with a placeholder\n",
        "\n",
        "for selected_model in MODEL_NAME_LIST:\n",
        "    print(f\"\\n===== Starting processing for Model: {selected_model} =====\")\n",
        "    for run_number in range(1, 2): # Just one run for prediction\n",
        "        print(f\"\\n--- Model: {selected_model}, Run: {run_number} ---\")\n",
        "        print(f\"Predicting details into columns: {', '.join(output_columns)}\")\n",
        "\n",
        "        print(f\"\\nProcessing {len(df)} rows using Google Gemini model: {selected_model} (Run {run_number})...\")\n",
        "\n",
        "        total_rows = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            print(f\"\\n--- Processing row {index + 1} of {total_rows} (Model: {selected_model}, Run: {run_number}) ---\")\n",
        "            facts = str(row[FACTS_COLUMN]) if pd.notna(row[FACTS_COLUMN]) else \"\"\n",
        "\n",
        "            # Get the predictions from Google Gemini\n",
        "            predictions = get_gemini_predictions(facts, selected_model)\n",
        "\n",
        "            # Update the DataFrame with the predicted values\n",
        "        # Update the DataFrame with the predicted values, taking only the first element if it's a list/tuple\n",
        "            for key, default_val in [\n",
        "                (\"Predicted_ID\", \"Error\"),\n",
        "                (\"Predicted_Year\", \"Error\"),\n",
        "                (\"Predicted_Plaintiff\", \"Error\"),\n",
        "                (\"Predicted_Defendent\", \"Error\")\n",
        "            ]:\n",
        "                predicted_val = predictions.get(key, default_val)\n",
        "                if isinstance(predicted_val, (list, tuple)) and len(predicted_val) > 0:\n",
        "                    df.loc[index, key] = predicted_val[0]\n",
        "                else:\n",
        "                    df.loc[index, key] = predicted_val\n",
        "\n",
        "            print(f\"Row {index + 1}: Predicted ID: {predictions.get('Predicted_ID', 'Error')}, \"\n",
        "                    f\"Year: {predictions.get('Predicted_Year', 'Error')}, \"\n",
        "                    f\"Plaintiff: {predictions.get('Predicted_Plaintiff', 'Error')}, \"\n",
        "                    f\"Defendant: {predictions.get('Predicted_Defendent', 'Error')}\")\n",
        "\n",
        "            if index < total_rows - 1:\n",
        "                if MAIN_LOOP_DELAY_SECONDS > 0:\n",
        "                    print(f\"Waiting for {MAIN_LOOP_DELAY_SECONDS} second(s) before next row...\")\n",
        "                    time.sleep(MAIN_LOOP_DELAY_SECONDS)\n",
        "\n",
        "        print(f\"\\n--- Finished Run {run_number} for Model: {selected_model} ---\")\n",
        "\n",
        "        # Save the DataFrame after each run (or after all runs if not looping)\n",
        "        print(f\"Saving results to {OUTPUT_EXCEL_FILE}...\")\n",
        "        try:\n",
        "            df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
        "            print(f\"Successfully saved results to: {OUTPUT_EXCEL_FILE}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results to Excel file: {e}\")\n",
        "\n",
        "    print(f\"\\n===== Finished all runs for Model: {selected_model} =====\")\n",
        "\n",
        "print(\"\\nAll processing complete. Script finished.\")"
      ],
      "metadata": {
        "id": "tI3iOk4nWU1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}