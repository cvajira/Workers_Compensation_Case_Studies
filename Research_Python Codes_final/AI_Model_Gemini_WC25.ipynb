{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3skPK9heVGJU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sxUxWTLgVWsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_EXCEL_FILE1 =\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_facts_AI.xlsx\"\n",
        "OUTPUT_EXCEL_FILE2 =\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_facts_gemini_simp_gemini_1.5_flash_002.xlsx\""
      ],
      "metadata": {
        "id": "96GxltrMVcig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai google-api-core pandas==2.2.2"
      ],
      "metadata": {
        "id": "ZlLw5VWOVs1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import time\n",
        "from google.api_core import exceptions # Import for specific exceptions\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "LYqW92WZVtq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "iFWB-wW5VwCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=api_key)\n",
        "# 2. Specify the Gemini Model\n",
        "# Use the latest appropriate 'flash' model identifier (e.g., gemini-1.5-flash-latest)\n",
        "# MODEL_NAME_LIST = [\"gemini-1.5-pro\", \"gemini-2.0-flash\", gemini-1.5-flash-002]\n",
        "MODEL_NAME_LIST = [\"gemini-1.5-flash-002\"] # Or \"gemini-1.5-flash-latest\"\n",
        "\n",
        "# 3. Excel File Paths\n",
        "INPUT_EXCEL_FILE = INPUT_EXCEL_FILE1  # Replace with your input file name\n",
        "OUTPUT_EXCEL_FILE = OUTPUT_EXCEL_FILE2 # Changed output name slightly\n",
        "\n",
        "# 4. Column Names (Adjust if different in your Excel)\n",
        "FACTS_COLUMN = \"Annonymized_Facts\"\n",
        "DECISION_COLUMN = \"AI_Decision1\"\n",
        "\n",
        "# 5. Time Delays (in seconds)\n",
        "# Delay AFTER processing each row (in the main loop)\n",
        "MAIN_LOOP_DELAY_SECONDS = 0\n",
        "# Delay BEFORE each API call (inside the function) - ADDED\n",
        "API_CALL_DELAY_SECONDS = 0  # <<< Adjust this value as needed (e.g., 1, 2, 3)\n",
        "\n",
        "\n",
        "# --- Create Gemini Model Instance (ONCE) ---\n",
        "generation_config = {\n",
        "  \"temperature\": 0,\n",
        "  \"top_p\": None,\n",
        "  \"top_k\": 1,\n",
        "  \"max_output_tokens\": 5,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# --- Function to get decision from Gemini ---\n",
        "def get_gemini_decision(facts_text, model_instance):\n",
        "    \"\"\"\n",
        "    Sends facts to Gemini and asks for a win (1) or loss (0) decision.\n",
        "    Includes a delay before making the API call.\n",
        "\n",
        "    Args:\n",
        "        facts_text (str): The text from the 'Facts' column.\n",
        "        model_instance: The pre-configured genai.GenerativeModel instance.\n",
        "\n",
        "    Returns:\n",
        "        int: 1 for predicted plaintiff win, 0 for predicted plaintiff loss,\n",
        "             -1 if an error occurred or decision couldn't be parsed.\n",
        "    \"\"\"\n",
        "    if not model_instance:\n",
        "        print(\"Error: Model instance is not available.\")\n",
        "        return -1\n",
        "\n",
        "    if not facts_text or not isinstance(facts_text, str) or len(facts_text.strip()) == 0:\n",
        "        print(\"Warning: Empty or invalid facts text provided.\")\n",
        "        return -1\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following legal case facts. Based solely on these facts, predict whether the plaintiff likely won or lost the case.\n",
        "\n",
        "    Respond ONLY with the number '1' if the plaintiff likely won.\n",
        "\n",
        "    Respond ONLY with the number '0' if the plaintiff likely lost.\n",
        "\n",
        "    Do NOT provide any explanation, commentary, or any text other than '1' or '0'.\n",
        "\n",
        "    Facts:\n",
        "    ---\n",
        "    {facts_text}\n",
        "    ---\n",
        "\n",
        "    Decision (1 for win, 0 for loss):\"\"\"\n",
        "\n",
        "    try:\n",
        "        # --- ADDED DELAY before API Call ---\n",
        "        print(f\"Waiting for {API_CALL_DELAY_SECONDS} second(s) before API call...\")\n",
        "        time.sleep(API_CALL_DELAY_SECONDS)\n",
        "        # ------------------------------------\n",
        "\n",
        "        # Call generate_content on the *shared* model instance\n",
        "        print(\"Making API call...\") # Added print statement\n",
        "        response = model_instance.generate_content(prompt)\n",
        "        print(\"API call complete.\") # Added print statement\n",
        "\n",
        "        # Attempt to parse the response\n",
        "        decision_text = response.text.strip()\n",
        "        if decision_text == '1':\n",
        "            return 1\n",
        "        elif decision_text == '0':\n",
        "            return 0\n",
        "        else:\n",
        "            print(f\"Warning: Could not parse decision from response: '{decision_text}' for facts: '{facts_text[:100]}...'\")\n",
        "            return -1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Gemini API call for facts: '{facts_text[:100]}...'. Error: {e}\")\n",
        "        # Check for prompt feedback in case of blocks\n",
        "        try:\n",
        "             # Check response object exists and has prompt_feedback attribute\n",
        "             if 'response' in locals() and response and hasattr(response, 'prompt_feedback') and response.prompt_feedback:\n",
        "                  print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
        "        except Exception as feedback_e:\n",
        "             print(f\"Could not retrieve prompt feedback: {feedback_e}\")\n",
        "        return -1\n",
        "\n",
        "# --- Main Processing Logic ---\n",
        "\n",
        "print(f\"\\nReading Excel file: {INPUT_EXCEL_FILE}\")\n",
        "try:\n",
        "    df = pd.read_excel(INPUT_EXCEL_FILE)\n",
        "    print(f\"Successfully read {len(df)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file not found at {INPUT_EXCEL_FILE}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading Excel file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Check for input column\n",
        "if FACTS_COLUMN not in df.columns:\n",
        "    print(f\"Error: Column '{FACTS_COLUMN}' not found in the Excel file.\")\n",
        "    exit()\n",
        "# main function############\n",
        "\n",
        "for selected_model in MODEL_NAME_LIST:\n",
        "    print(f\"\\n===== Starting processing for Model: {selected_model} =====\")\n",
        "    try:\n",
        "      print(f\"Creating Gemini model instance: {selected_model}\")\n",
        "      shared_model_instance = genai.GenerativeModel(\n",
        "          model_name=selected_model,\n",
        "          generation_config=generation_config,\n",
        "          safety_settings=safety_settings\n",
        "          )\n",
        "      print(\"Model instance created successfully.\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error creating Gemini model instance: {e}\")\n",
        "      shared_model_instance = None\n",
        "      exit()\n",
        "\n",
        "    for run_number in range(1, 2): # Loop 1 to 3 (inclusive)\n",
        "        # Create dynamic decision column name based on model and run number\n",
        "        # e.g., \"gpt-3.5-turbo-0125_AI_Decision1\", \"gpt-3.5-turbo-0125_AI_Decision2\", etc.\n",
        "        current_decision_column = f\"{selected_model}_AI_Decision{run_number}\"\n",
        "\n",
        "        print(f\"\\n--- Model: {selected_model}, Run: {run_number} ---\")\n",
        "        print(f\"Outputting decisions to column: {current_decision_column}\")\n",
        "\n",
        "        # Ensure the Decision column for this specific run/model exists, initialize if not\n",
        "        if current_decision_column not in df.columns:\n",
        "            print(f\"Initializing column: {current_decision_column}\")\n",
        "            df[current_decision_column] = -1 # Initialize with a placeholder\n",
        "        else:\n",
        "            print(f\"Column '{current_decision_column}' already exists. Values will be updated/overwritten for this run.\")\n",
        "\n",
        "        print(f\"\\nProcessing {len(df)} rows using Gemini model: {selected_model} (Run {run_number})...\")\n",
        "\n",
        "        # --- Iterate and call Gemini API for each row ---\n",
        "        total_rows = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            print(f\"\\n--- Processing row {index + 1} of {total_rows} (Model: {selected_model}, Run: {run_number}) ---\")\n",
        "            facts = str(row[FACTS_COLUMN]) if pd.notna(row[FACTS_COLUMN]) else \"\"\n",
        "\n",
        "            # Get the decision from Gemini using the shared model\n",
        "            decision = get_gemini_decision(facts, shared_model_instance)\n",
        "\n",
        "            # Update the DataFrame in the dynamically named column\n",
        "            df.loc[index, current_decision_column] = decision\n",
        "            print(f\"Row {index + 1}: Facts processed for column '{current_decision_column}'. Predicted Decision: {decision}\")\n",
        "\n",
        "            # Optional: Delay AFTER processing each row\n",
        "            if index < total_rows - 1: # Avoid sleeping after the last row of this run\n",
        "                if MAIN_LOOP_DELAY_SECONDS > 0:\n",
        "                    print(f\"Waiting for {MAIN_LOOP_DELAY_SECONDS} second(s) before next row...\")\n",
        "                    time.sleep(MAIN_LOOP_DELAY_SECONDS)\n",
        "\n",
        "        print(f\"\\n--- Finished Run {run_number} for Model: {selected_model} ---\")\n",
        "\n",
        "        # --- MODIFIED: Save the DataFrame at the end of each inner loop (run) ---\n",
        "        print(f\"Saving results to {OUTPUT_EXCEL_FILE} after Run {run_number} for Model {selected_model}...\")\n",
        "        try:\n",
        "            df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
        "            print(f\"Successfully saved results to: {OUTPUT_EXCEL_FILE}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results to Excel file after Run {run_number} for Model {selected_model}: {e}\")\n",
        "        # --- END OF SAVE MODIFICATION FOR INNER LOOP ---\n",
        "\n",
        "    print(f\"\\n===== Finished all runs for Model: {selected_model} =====\")\n",
        "# --- END OF MODIFIED SECTION ---\n",
        "\n",
        "print(\"\\nAll processing complete.\") # This message now indicates all models and all their runs are done.\n",
        "\n",
        "# The final save after all processing is now removed as it's done per run.\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "id": "tI3iOk4nWU1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}