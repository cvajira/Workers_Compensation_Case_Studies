{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr_Vd4P1qaqp"
      },
      "outputs": [],
      "source": [
        "# Data was pre processed for TF-IDF, Word2Vec and BERT methods. For LLM models such as Gemini and GPT, annonymized data used without any preprocessing (since they have builtin preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T0J_0p4irNiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MIiI_foOq6Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_EXCEL_FILE =\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready3.xlsx\"\n",
        "OUTPUT_EXCEL_FILE = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready4.xlsx\""
      ],
      "metadata": {
        "id": "DiF6oomerbR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(INPUT_EXCEL_FILE)"
      ],
      "metadata": {
        "id": "zaBHNvERrhzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#column names\n",
        "df.columns"
      ],
      "metadata": {
        "id": "gmHgb3fxrpi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create two columns to hold pre processed Issues and Finding of Facts\n",
        "df['Preprocessed_Facts']=df['Annonymized_Facts']\n",
        "df['Preprocesses_Issues']=df['Annonymized_Issues']"
      ],
      "metadata": {
        "id": "waXgCpwrs0NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "oRsDrYGztYB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess Finding of Facts\n",
        "col_to_clean='Preprocessed_Facts'\n",
        "df[col_to_clean]=df[col_to_clean].astype(str)\n",
        "df[col_to_clean]=df[col_to_clean].apply(lambda s: s.strip())\n",
        "df[col_to_clean][0]"
      ],
      "metadata": {
        "id": "EEcoWMzosTNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wordcloud"
      ],
      "metadata": {
        "id": "0uIfByqCtsEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ji3FOsKctvNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "uKUl58s6uF7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stopwords, stemmer, and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(raw):\n",
        "  # Lowercase\n",
        "    result=raw.lower()\n",
        "    result = re.sub(\"<[a][^>]*>(.+?)</[a]>\", 'Link.', raw) #This line targets HTML anchor tags (<a> tags) which are used to create hyperlinks.\n",
        "    result = re.sub('&gt;', \"\", result) #This line replaces the HTML entity &gt; (greater than symbol, >) with an empty string, effectively removing it from the text.\n",
        "    result = re.sub('&#x27;', \"'\", result)#This replaces the HTML entity &#x27; (apostrophe, ') with an actual apostrophe character.\n",
        "    result = re.sub('&quot;', '\"', result) #This replaces the HTML entity &quot; (quotation mark, \") with an actual quotation mark character.\n",
        "    result = re.sub('&#x2F;', ' ', result) #This replaces the HTML entity &#x2F; (forward slash, /) with a space.\n",
        "    result = re.sub('<p>', ' ', result) #This replaces paragraph tags (<p>) with a space.\n",
        "    result = re.sub('</i>', '', result) #This removes closing italics tags (</i>).\n",
        "    result = re.sub('&#62;', '', result) #This replaces the HTML entity &#62; (greater than symbol, >) with an empty string\n",
        "    result = re.sub('<i>', ' ', result) #This replaces opening italics tags (<i>) with a space.\n",
        "    result = re.sub(\"\\n\", '', result) #This removes newline characters (\\n) from the text.\n",
        "    result = re.sub(\"<.*?>\", \"\", result)  # Remove any remaining HTML tags\n",
        "    result = re.sub(r'http\\S+', '', result)  # Remove URLs\n",
        "    result = re.sub(r'\\d+', '', result)  # Remove digits\n",
        "    result = re.sub(r'[^\\w\\s]', '', result)   # Removes all punctuation\n",
        "    result = re.sub(' +', ' ', result)#to unify multiple spaces into a single space\n",
        "\n",
        "    # Remove stopwords\n",
        "    result = ' '.join([word for word in result.split() if word not in stop_words]) # Here we split the sentence, remove stopwords, then join it again to make the senetence\n",
        "\n",
        "    # Stemming\n",
        "    word_tokens = nltk.word_tokenize(result)\n",
        "    stemmed_words = [snowball_stemmer.stem(word) for word in word_tokens]\n",
        "    result = ' '.join(stemmed_words)\n",
        "\n",
        "    # Lemmatization\n",
        "    word_tokens = nltk.word_tokenize(result)\n",
        "    lemmatized_words = [lemma.lemmatize(word) for word in word_tokens]\n",
        "    result = ' '.join(lemmatized_words)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "R5VmWYiIP-UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[col_to_clean]=df[col_to_clean].apply(lambda s: clean_text(s))"
      ],
      "metadata": {
        "id": "WwrqclFvuAKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[col_to_clean][0]"
      ],
      "metadata": {
        "id": "2iXYrosUvfky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_to_clean='Preprocesses_Issues'\n",
        "df[col_to_clean]=df[col_to_clean].astype(str)\n",
        "df[col_to_clean]=df[col_to_clean].apply(lambda s: s.strip())\n",
        "df[col_to_clean][0]"
      ],
      "metadata": {
        "id": "eULe5uauvu1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[col_to_clean]=df[col_to_clean].apply(lambda s: clean_text(s))"
      ],
      "metadata": {
        "id": "Bq1llF4Zv511"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[col_to_clean][0]"
      ],
      "metadata": {
        "id": "EYZN4meEv_th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "78WLjJdkwCdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop 'Issues', 'Findings of Fact','Annonymized_Facts', 'Annonymized_Issues',\n",
        "df.drop(['Issues', 'Findings of Fact','Annonymized_Facts', 'Annonymized_Issues'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "9ezeijsKwGJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(OUTPUT_EXCEL_FILE, index=False)"
      ],
      "metadata": {
        "id": "AZlceBGXwSe9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}