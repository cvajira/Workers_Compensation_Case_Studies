{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Using TF embedding approach and RF, GB, and XGB to classify Decision according to Issue"
      ],
      "metadata": {
        "id": "_ZPHSejP4Oig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ETiatDyM5EBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "451EddMf9uEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiesZb2X34mC"
      },
      "outputs": [],
      "source": [
        "INPUT_EXCEL_FILE1 = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_Issues_train.xlsx\"\n",
        "INPUT_EXCEL_FILE2 = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_Issues_test.xlsx\"\n",
        "INPUT_EXCEL_FILE3 = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_Facts_train.xlsx\"\n",
        "INPUT_EXCEL_FILE4 = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_Facts_test.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load training and testing files for Issues\n",
        "df_train_issues = pd.read_excel(INPUT_EXCEL_FILE1)\n",
        "df_test_issues = pd.read_excel(INPUT_EXCEL_FILE2)"
      ],
      "metadata": {
        "id": "PN1iWITW5R2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_issues.columns"
      ],
      "metadata": {
        "id": "H8DoQILP5dRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_issues.shape\n",
        "#should be 4882"
      ],
      "metadata": {
        "id": "CJtQMX_-5iuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_issues.shape\n",
        "#should be 1221"
      ],
      "metadata": {
        "id": "LCU7SfHg5m9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check unique values in 'Decision' column\n",
        "df_train_issues['Decision'].unique()"
      ],
      "metadata": {
        "id": "X8gi7Oyg5tAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the distribution of Decsion variable in train and test as percentage\n",
        "df_train_issues['Decision'].value_counts(normalize=True)\n",
        "#should be about 63%-37%"
      ],
      "metadata": {
        "id": "62b08Ayj52jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_issues['Decision'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "Kp9G-OQW7Sfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = df_train_issues['Preprocesses_Issues'].tolist()\n",
        "test_text = df_test_issues['Preprocesses_Issues'].tolist()\n",
        "\n",
        "train_labels = df_train_issues['Decision'].tolist()\n",
        "test_labels = df_test_issues['Decision'].tolist()"
      ],
      "metadata": {
        "id": "A3L1AEca7eq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "dWGIsDYc8BY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download tokenizer models"
      ],
      "metadata": {
        "id": "JBJuDRz58IiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "1fjwMxeB8OXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenize_text(texts):\n",
        "    \"\"\"\n",
        "    Tokenize a list of sentences into words.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List of raw sentences (strings)\n",
        "\n",
        "    Returns:\n",
        "    - List of tokenized sentences (lists of words)\n",
        "    \"\"\"\n",
        "    return [word_tokenize(text.lower()) for text in texts]  # Lowercase and tokenize\n",
        "\n",
        "# Function to convert text data into TF-IDF features\n",
        "def loadDataTfidf(X_train, X_test, max_features=75000):\n",
        "    \"\"\"\n",
        "    Convert the training and test data into TF-IDF-based numerical features.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: List of raw training sentences\n",
        "    - X_test: List of raw test sentences\n",
        "    - max_features: Maximum number of features (vocabulary size) for TF-IDF\n",
        "\n",
        "    Returns:\n",
        "    - X_train_tfidf: TF-IDF-based numerical features for training data\n",
        "    - X_test_tfidf: TF-IDF-based numerical features for test data\n",
        "    \"\"\"\n",
        "    # Step 1: Initialize TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "    # Step 2: Fit the vectorizer on the training data and transform both train and test data\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    print(f\"TF-IDF with {X_train_tfidf.shape[1]} features\")\n",
        "\n",
        "    return X_train_tfidf, X_test_tfidf\n"
      ],
      "metadata": {
        "id": "gYWE82Ir8ToO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numerical features using TF-IDF\n",
        "X_train_tfidf, X_test_tfidf = loadDataTfidf(train_text, test_text, max_features=100)"
      ],
      "metadata": {
        "id": "OIKmv7Kg8X-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf[0]"
      ],
      "metadata": {
        "id": "WDC-uSsp8yn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train_tfidf)"
      ],
      "metadata": {
        "id": "jVD0yf9x82-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=train_labels\n",
        "y_test=test_labels"
      ],
      "metadata": {
        "id": "OEl_Q2t_9Yy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.metrics import specificity_score\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n"
      ],
      "metadata": {
        "id": "iSbGORIR9AOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a code for calculating model performance metrics and deliver it as one single data frame\n",
        "def model_metrics(model, predictors, targets):\n",
        "  pred=model.predict(predictors)\n",
        "  pred_prob=model.predict_proba(predictors)[:,1]\n",
        "\n",
        "  acc=accuracy_score(targets, pred)\n",
        "  rec=recall_score(targets, pred)\n",
        "  spec=specificity_score(targets, pred, average='binary')\n",
        "  prec=precision_score(targets, pred)\n",
        "  f1=f1_score(targets, pred)\n",
        "  auc=roc_auc_score(targets,pred_prob)\n",
        "\n",
        "  df_metrics=pd.DataFrame({}, index=['Metrics'])\n",
        "  df_metrics['Accuracy']=acc\n",
        "  df_metrics['Recall']=rec\n",
        "  df_metrics['Specificity']=spec\n",
        "  df_metrics['Precision']=prec\n",
        "  df_metrics['F1']=f1\n",
        "  df_metrics['AUC']=auc\n",
        "\n",
        "  return df_metrics"
      ],
      "metadata": {
        "id": "V85TEBYc9Ijj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and Display confusion matrix\n",
        "\n",
        "def display_confusion_matrix(model, predictors, targets):\n",
        "  pred=model.predict(predictors)\n",
        "  cm=confusion_matrix(targets, pred)\n",
        "  #cm_percentage=cm.astype('float')/cm.sum()*100\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "  disp.plot(cmap=plt.cm.Blues)\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "IMYIBlBB9LTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Random Forest Classifier"
      ],
      "metadata": {
        "id": "n0LFGs2B-kfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 2: Train the model using X_train_tfidf and y_train\n",
        "rf_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "test_performance=model_metrics(rf_classifier, X_test_tfidf, y_test)\n",
        "test_performance"
      ],
      "metadata": {
        "id": "vlNarfEg9OTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_confusion_matrix(rf_classifier, X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "t01le_iY9iHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune the model\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import metrics\n",
        "# defining model\n",
        "Model =RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50,110,25],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [0.3, 0.4, 0.5, 'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv_RF = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv_RF.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv_RF.best_params_,randomized_cv_RF.best_score_))"
      ],
      "metadata": {
        "id": "vuznsOCm94dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=randomized_cv_RF.best_params_\n",
        "tuned_RF=RandomForestClassifier(random_state=42, class_weight='balanced', **best_params)\n",
        "tuned_RF.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "jB4lsieq-UHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_performance_tunedRF=model_metrics(tuned_RF, X_test_tfidf, y_test)\n",
        "test_performance_tunedRF.round(4)"
      ],
      "metadata": {
        "id": "tlzAj2F4-cTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "Q1TFDOs0-qcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Step 2: Train the model using X_train_tfidf and y_train\n",
        "gb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "test_performance=model_metrics(gb_classifier, X_test_tfidf, y_test)\n",
        "test_performance"
      ],
      "metadata": {
        "id": "PKB83MOa-aR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_confusion_matrix(gb_classifier, X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "-krvbD7D-4DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "\n",
        "# To help with model building\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "Model =GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"subsample\":[0.7,0.9],\n",
        "    \"max_features\":[0.5,0.7,1],\n",
        "    \"max_depth\": [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv_GB = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv_GB.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv_GB.best_params_,randomized_cv_GB.best_score_))"
      ],
      "metadata": {
        "id": "LNa-6Hee_AnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=randomized_cv_GB.best_params_\n",
        "tuned_GB=GradientBoostingClassifier(random_state=42,  **best_params)\n",
        "tuned_GB.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "eKvIujM5_V7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_performance_tunedGB=model_metrics(tuned_GB, X_test_tfidf, y_test)\n",
        "test_performance_tunedGB.round(4)"
      ],
      "metadata": {
        "id": "0Te5k3Za_ZfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kunore2Q_fFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using XGBoost classifier"
      ],
      "metadata": {
        "id": "9HFAD_iK_gfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "o-dMLo6A_l00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Step 2: Train the model using X_train_tfidf and y_train\n",
        "xgb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "test_performance=model_metrics(xgb_classifier, X_test_tfidf, y_test)\n",
        "test_performance"
      ],
      "metadata": {
        "id": "YdFzLidV_ozY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# defining model\n",
        "Model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv_XGB = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv_XGB.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv_XGB.best_params_,randomized_cv_XGB.best_score_))"
      ],
      "metadata": {
        "id": "5pX_AAET_vWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params=randomized_cv_XGB.best_params_\n",
        "tuned_XGB=XGBClassifier(random_state=42,  **best_params)\n",
        "tuned_XGB.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "djk2rBhrAGbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_performance_tunedXGB=model_metrics(tuned_XGB, X_test_tfidf, y_test)\n",
        "test_performance_tunedXGB.round(4)"
      ],
      "metadata": {
        "id": "7mFwIGhMAKUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost- Using Oversampling Approach"
      ],
      "metadata": {
        "id": "It7DmBXgAOwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To oversample and undersample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "metadata": {
        "id": "Zo6X5HjoAR2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Oversampling, counts of label '1': {}\".format(sum(y_train)))\n",
        "print(\"Before Oversampling, counts of label '0': {} \\n\".format(len(y_train)-sum(y_train)))"
      ],
      "metadata": {
        "id": "wIW2ST-HAUKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For oversampling\n",
        "sm = SMOTE(\n",
        "    sampling_strategy=1, k_neighbors=5, random_state=1\n",
        ")  # Synthetic Minority Over Sampling Technique\n",
        "X_train_over, y_train_over = sm.fit_resample(\n",
        "  X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "N4__Xl08AXAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for undersampling\n",
        "\n",
        "rus = RandomUnderSampler(random_state=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "F9pxesg8Aaed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After Oversampling, counts of label '1': {}\".format(sum(y_train_over)))\n",
        "print(\"After Oversampling, counts of label '0': {} \\n\".format(len(y_train_over)-sum(y_train_over)))"
      ],
      "metadata": {
        "id": "_t56Cpz5AdfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After undersampling, counts of label '1': {}\".format(sum(y_train_un)))\n",
        "print(\"After undersampling, counts of label '0': {} \\n\".format(len(y_train_un)-sum(y_train_un)))"
      ],
      "metadata": {
        "id": "JXVGnKn4Ag1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversampling and XGB\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42,  **best_params)\n",
        "\n",
        "# Step 2: Train the model using X_train_tfidf and y_train\n",
        "xgb_classifier.fit(X_train_over, y_train_over)\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "test_performance_xgbover=model_metrics(xgb_classifier, X_test_tfidf, y_test)\n",
        "test_performance_xgbover.round(4)"
      ],
      "metadata": {
        "id": "nqThbnsHAkyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling and XGB\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42,  **best_params)\n",
        "\n",
        "# Step 2: Train the model using X_train_tfidf and y_train\n",
        "xgb_classifier.fit(X_train_un, y_train_un)\n",
        "\n",
        "# Step 3: Make predictions on the test set\n",
        "test_performance_xgbunder=model_metrics(xgb_classifier, X_test_tfidf, y_test)\n",
        "test_performance_xgbunder.round(4)"
      ],
      "metadata": {
        "id": "mH0ltJmXAp9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_performance_tunedRF.round(4))\n",
        "print(test_performance_tunedGB.round(4))\n",
        "print(test_performance_tunedXGB.round(4))\n",
        "print(test_performance_xgbover.round(4))\n",
        "print(test_performance_xgbunder.round(4))"
      ],
      "metadata": {
        "id": "NNZGZisQwRkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NSWzhkZ_iC8t"
      }
    }
  ]
}