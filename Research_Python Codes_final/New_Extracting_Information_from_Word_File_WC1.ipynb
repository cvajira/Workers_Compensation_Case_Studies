{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTpL51NkSxT-"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx pypandoc\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from docx import Document\n",
        "import pypandoc\n",
        "import glob # To find files matching a pattern\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Path to the folder containing the original Word files\n",
        "folder_path = '/content/drive/MyDrive/worker_comp/DSI2'  # Path to folder contains original docs"
      ],
      "metadata": {
        "id": "f5b8En5PPcme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert doc files to docx\n",
        "\n",
        "# Get all file names in the folder\n",
        "file_names = os.listdir(folder_path)\n",
        "\n",
        "# Filter out only the doc Word files (assuming Word files have extension .doc or .docx)\n",
        "doc_word_files = [file_name for file_name in file_names if file_name.endswith('.doc')]\n",
        "\n",
        "# Function to convert .doc to .docx\n",
        "def convert_doc_to_docx(doc_path):\n",
        "    docx_path = doc_path + 'x'  # Just append 'x' to the .doc file name to create .docx\n",
        "    pypandoc.convert_file(doc_path, 'docx', outputfile=docx_path)\n",
        "    return docx_path\n",
        "\n",
        "for file_name in doc_word_files:\n",
        "  doc_path = os.path.join(folder_path, file_name)\n",
        "  docx_path = convert_doc_to_docx(doc_path)"
      ],
      "metadata": {
        "id": "_jhct_Osakko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract dates from the first page of a document\n",
        "def extract_dates(doc_path):\n",
        "    if doc_path.endswith('.doc'):\n",
        "        doc_path = convert_doc_to_docx(doc_path)\n",
        "\n",
        "    document = Document(doc_path)\n",
        "    text = ''\n",
        "\n",
        "    # Extract text from the first page\n",
        "    for paragraph in document.paragraphs:\n",
        "        text += paragraph.text + '\\n'\n",
        "\n",
        "    # Search for dates in the format \"Month Day, Year\" (e.g., \"April 23, 2014\")\n",
        "    dates = re.findall(r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}', text)\n",
        "\n",
        "    # Assuming the first date found is the beginning date, and the second is the end date\n",
        "    beginning_date = dates[0] if dates else 'N/A'\n",
        "    end_date = dates[1] if len(dates) > 1 else 'N/A'\n",
        "\n",
        "    return beginning_date, end_date"
      ],
      "metadata": {
        "id": "a3Cr3C6LUzaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder containing the docx files\n",
        "folder_path = '/content/drive/MyDrive/worker_comp/DSI2'"
      ],
      "metadata": {
        "id": "9GDJzc83TEmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "# Assuming convert_doc_to_docx exists if needed, or handle .doc separately.\n",
        "# from your_module import convert_doc_to_docx # Example import\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_DIRECTORY = '/content/drive/MyDrive/worker_comp/DSI2' # <<< CHANGE THIS to the folder containing your DOCX files\n",
        "OUTPUT_EXCEL_FILE = '/content/drive/MyDrive/worker_comp/output_extracted_data.xlsx' # <<< CHANGE THIS (optional) to your desired output filename\n",
        "\n",
        "# --- Section Definitions ---\n",
        "# Define the sections and their variations (as provided previously)\n",
        "SECTION_VARIATIONS = {\n",
        "    \"Appearances\": ['A P E A R A N C E S', 'A P P E A R A N C E', 'A P P E A R A N C ES', 'A P P E A R A N C E S', 'A P P E A R A N C E S_2', 'A P P E A R A NC E S', 'A P P E A R AN C E S', 'A P P E A R ANCES', 'A P P E A RA N C E S', 'A P P E AR A N C E S', 'A P P E R A N C E S', 'A P P EA R A N C E S', 'A P P EA R AN C E S', 'A P P  E A R A N C E S', 'APPEARANCES', 'APPERARANCES'],\n",
        "    \"Stipulations\": ['ADDITIONAL STIPULATIONS', 'COMMON STIPULATIONS', 'FACTUAL STIPULATIONS', 'GENERAL STIPULATIONS', 'JOINT STIPULATIONS', 'JURISDICTIONAL AND STANDARD STIPULATIONS', 'PROCEDURAL STIPULATION', 'S T I P U L A T I O N S', 'STIPUALTIONS', 'STIPULATED FACTS', 'STIPULATION', 'STIPULATION OF FACTS', 'STIPULATION OF THE FACTS', 'STIPULATION OF UNDISPUTED FACTS', 'STIPULATIONS', 'STIPULATIONS BETWEEN THE PARTIES', 'STIPULATIONS BY DEFENDANTS', 'STIPULATIONS COMMON TO BOTH CLAIMS', 'STIPULATIONS OFFERED BY DEFENDANT', 'STIPULATIONS OF FACT', 'STIPULATIONS OF FACTS', 'STIPULATIONS POST HEARING', 'STIPULATIONS_2', 'STIPULATONS', 'SUPPLEMENTAL STIPULATION'],\n",
        "    \"Exhibits\": ['ADDITIONAL EXHIBITS', 'ADMITTED EXHIBITS', 'DEMONSTRATIVE EVIDENCE', 'DOCUMENTARY EVIDENCE', 'E X H I B I T S', 'EVIDENCE ADMITTED', 'EXBIBITS', 'EXHIB', 'EXHIBIT', 'EXHIBIT A', 'EXHIBITS', 'EXHIBITS ADMITTED', 'EXHIBITS AND DEPOSITIONS', 'EXHIBITS INTRODUCED AT THE HEARING', 'EXHIBITS INTRODUCED INTO THE RECORD', 'EXHIBITS MATTERS', 'EXHIBITS RECEIVED INTO EVIDENCE', 'EXHIBITS_2', 'EXHIBITS_3', 'HEARING DOCUMENTS', 'HEARING EXHIBITS', 'MEDICAL RECORDS ADMITTED INTO EVIDENCE', 'OTHER EXHIBITS', 'OTHER EXHIBITS INTRODUCED AT THE HEARING', 'STIPULATED DOCUMENTS', 'STIPULATED EVIDENCE', 'STIPULATED EXHIB', 'STIPULATED EXHIBIT', 'STIPULATED EXHIBIT I', 'STIPULATED EXHIBIT II', 'STIPULATED EXHIBITS', 'STIPULATED EXHIBITS_2', 'STIPULATED RECORDS', 'SUPPLEMENTAL EXHIBIT'],\n",
        "    \"Issues\": ['ADDING NEW ISSUE FOR HEARING', 'ADDITIONAL ISSUES INCLUDE', 'CONTESTED ISSUES', 'CONTESTED ISSUES OF THE DEFENDANT', 'CONTESTED ISSUES OF THE PLAINTIFF', 'DEFENDANT GABRIEL BUILDERS ISSUES', 'DEFENDANT TRAVELERS ISSUES', 'DEFENDANTS ISSUES', 'ISSUE', 'ISSUE FOR DECISION', 'ISSUE FOR DETERMINATION', 'ISSUE FOR DETERMINATION INCLUDES', 'ISSUE FOR HEARING', 'ISSUE PRESENTED', 'ISSUE TO BE DECIDED', 'ISSUE TO BE DETERMINED', 'ISSUES', 'ISSUES DETERMINED', 'ISSUES FOR DECISION', 'ISSUES FOR DETERMINATION', 'ISSUES FOR DETERMINATION INCLUDE THE FOLLOWING', 'ISSUES FOR DETERMINATON', 'ISSUES FOR HEARING', 'ISSUES FOR RESOLUTION', 'ISSUES IN DISPUTE', 'ISSUES OF DEFENDANTS', 'ISSUES OF PLAINTIFF', 'ISSUES PRESENTED', 'ISSUES TO BE DECIDED', 'ISSUES TO BE DETERMINED', 'ISSUES TO BE DETRMINED', 'ISSUES TO BE DTERMINED', 'ISSUES TO BE RESOLVED', 'ISSUES TO BE  DETERMINED', 'ISSUES_2', 'ISSUED TO BE DETERMINED', 'ISSURES FOR DETERMINATION', 'ISSUES TO BE DETERMINED', 'ITEMS TO BE DETERMINED', 'MEDICAL ISSUE TO BE DETERMINED', 'PLAINTIFF ISSUES', 'PROPOSED ISSUE FOR DETERMINATION', 'PROPOSED ISSUES FOR DETERMINATION', 'PSYCHOLOGICAL ISSUE', 'QUESTIONS PRESENTED', 'RULING ON ADDITIONAL ISSUE RAISED BY PLAINTIFF IN CONTENTIONS', 'RULING ON ISSUES PRESENTED', 'RULINGS ON ISSUES PRESENTED', 'STATEMENT OF ISSUES', 'STIPULATED CONTESTED ISSUES', 'STIPULATED ISSUES', 'STIPULATED ISSUES PRESENTED', 'WAGE ISSUE'],\n",
        "    \"Findings of Fact\": ['ADDITIONAL FINDING OF FACT', 'ADDITIONAL FINDINGS OF FACT', 'F I N D I N G S  OF FACT', 'FACTS', 'FINDING AND FACTS', 'FINDING OF FACT', 'FINDING OF FACTS', 'FINDINGS  OF FACT', 'FINDINGS FACT', 'FINDINGS OF FACT', 'FINDINGS OF FACT BASED ON THE ENTIRE RECORD', 'FINDINGS OF FACT FACTS', 'FINDINGS OF FACT_2', 'FINDINGS OF FACTS', 'FINDINGS OF THE FACT', 'FINDINGS OF THE FACTS', 'FINDINGS OFFACT', 'FINDINGSOFFACT', 'FINDINS OF FACT', 'FINDNGS OF FACT', 'FINDS OF FACTS', 'FURTHER FINDINGS OF FACT BY CONSENT', 'STATEMENT OF FACTS', 'STATEMENT OF STIPULATED FACTS', 'STATEMENT OF THE FACTS', 'STIPULATED FACTUAL FINDINGS', 'STIPULATED FINDINGS OF FACT', 'STIPULATED FINDINGS OF FACTS'],\n",
        "    \"Conclusions of Law\": ['AGREED UPON CONCLUSIONS OF LAW', 'CONCLUSION AT LAW', 'CONCLUSION OF LAW', 'CONCLUSION OF THE LAW', 'CONCLUSIONS', 'CONCLUSIONS LAW', 'CONCLUSIONS OF LAW', 'CONCLUSIONS OF LAW PERTAINING TO THE ASBESTOS CLAIM', 'CONCLUSIONS OF LAW PERTAINING TO THE EYE CLAIM', 'CONCLUSIONS OF LAW_2', 'CONCLUSIONS OF L A W', 'CONCLUSIONS  OF  LAW', 'CONCLUSIONS  OF LAW', 'CONCLUSIONSOFLAW', 'CONCULSIONS OF LAW', 'CONLCUSIONS OF LAW', 'CONSLUSIONS OF LAW', 'CONCLUSUONS OF LAW', 'STIPULATED CONCLUSIONS OF LAW'],\n",
        "    \"Order/Award\": ['AWARD & ORDER', 'AWARD AND ORDER', 'AWARD  &  ORDER','A W A R D', 'A W A R D S', 'A W A R D_2', 'A W A RD', 'A W AR D', 'A  W A R D', 'A WA R D', 'A W A  D', 'A W A R  D', 'AWARD', 'INTERLOCATORY AWARD', 'O R D E R', 'O R D E R E D', 'O R D ER', 'O R DE R', 'ORDER', 'ORDER BY DEPUTY COMMISSIONER LORI GAINES', 'ORDERED', 'ORDERS','O R D E R & A W A R D', 'O R D E R  &  A W A RD', 'O R D E R  &  A W A R D', 'ORDER & AWARD', 'ORDER AND AWARD'],\n",
        "    \"Combined Sections\": [ 'CONCLUSIONS AND ORDER', 'CONCLUSIONS OF LAW AND AWARD', 'FINDING OF FACT AND CONCLUSION OF LAW', 'FINDINGS OF FACT & CONCLUSIONS OF LAW', 'FINDINGS OF FACT AND CONCLUSIONS OF LAW',  'STIPULATIONS & FINDINGS OF FACT', 'STIPULATIONS AND FINDINGS OF FACT', 'STIPULATIONS CONSENT ORDER AND AWARD'],\n",
        "    \"Evidentiary Rulings/Matters\": ['ADDITIONAL EVIDENTIARY PLEADINGS TO BE INCLUDED IN THE RECORD', 'ADDITIONAL EVIDENTIARY RULING', 'ADDITIONAL EVIDENTIARY RULINGS', 'EVIDENCIARY MATTER', 'EVIDENCIARY RULING', 'EVIDENCIARY RULINGS', 'EVIDENRTIARY MATTERS', 'EVIDENTARY MATTERS', 'EVIDENTARY RULING', 'EVIDENTIARY AND OTHER RULINGS', 'EVIDENTIARY DOCUMENTS', 'EVIDENTIARY ISSUE', 'EVIDENTIARY ISSUES', 'EVIDENTIARY MATER', 'EVIDENTIARY MATTER', 'EVIDENTIARY MATTERS', 'EVIDENTIARY MATTTERS', 'EVIDENTIARY NOTE', 'EVIDENTIARY RULES', 'EVIDENTIARY RULING', 'EVIDENTIARY RULING REGARDING THE TESTIMONY OF AL GORROD', 'EVIDENTIARY RULINGS', 'EVIDENTIARY RULINGS ON OBJECTIONS RAISED DURING DEPOSITION', 'EVIDENTIARY RULINGS ON OBJECTIONS RAISED DURING DEPOSITIONS', 'EVIDENTIARY RULINGS_2', 'EVIDENTIATY MATTERS', 'EVIDENTIRAY MATTERS', 'OBJECTIONS', 'OBJECTIONS AND ADMISSION OF EXHIBITS', 'OBJECTIONS AND EVIDENTIARY RULINGS', 'OBJECTIONS RAISED DURING DEPOSITIONS', 'RULING ON EVIDENCE MATTERS', 'RULING ON EVIDENTIARY ISSUE', 'RULING ON EVIDENTIARY MATTER', 'RULING ON EVIDENTIARY MATTERS', 'RULING ON EVIDENTIARY OBJECTIONS', 'RULING ON HEARSAY OBJECTIONS', 'RULING ON OBJECTIONS', 'RULINGS ON DEPOSITION OBJECTIONS', 'RULINGS ON EVIDENTIARY MATTERS', 'RULINGS ON EVIDENTIARY MATTERS AND FURTHER ORDERS', 'RULINGS ON OBJECTIONS AND ADMISSION OF EXHIBITS', 'RULINGS ON THE EVIDENCE', 'RULES ON EVIDENTIARY MATTERS'],\n",
        "    \"Depositions/Testimony\": ['DEPOSITION', 'DEPOSITION OF CAROL HARRIS', 'DEPOSITION OF KELLY LANCE', 'DEPOSITION OF STEPHEN CARPENTER', 'DEPOSITION TAKEN', 'DEPOSITION TESTIMONY', 'DEPOSITION TRANSCRIPTS', 'DEPOSITIONS', 'DEPOSITIONS UPON ORAL EXAMINATION', 'DEPOSITIONS_2', 'DEPOSITONS', 'DEPONENTS', 'DESPOSITIONS', 'EXPERT OPINION', 'EXPERT TESTIMONY', 'EXPERT TESTIMONY RULING', 'EXPERT WITNESS WRITTEN RESPONSES', 'EXPERTS', 'LIST OF PERSONS DEPOSED', 'MEDICAL OPINION CORRESPONDENCE SUBMITTED IN LIEU OF DEPOSITIONS', 'MEDICAL QUESTIONNAIRE RESPONSES', 'MEDICAL QUESTIONNAIRES', 'MEDICAL WITNESSES', 'PERSONS DEPOSED', 'RESPONSES TO MEDICAL QUESTIONNAIRE & DEPOSITION', 'TESTIMONIAL EVIDENCE', 'TESTIMONY BY DEPOSITION', 'WITNESSES'],\n",
        "    \"Procedural History/Matters\": ['ADDITIONAL PROCEDURAL HISTORY', 'BACKGROUND', 'CASE HISTORIES', 'CASE HISTORY', 'FACTUAL AND PROCEDURAL HISTORY', 'HISTORY OF CASE', 'INTRODUCTION', 'INTRODUCTORY PARAGRAPH', 'JUDICIAL HISTORY', 'PARTIAL PROCEDURAL HISTORY', 'PRELIMINARY MATTERS', 'PRELIMINARY RULINGS', 'PRIOR OPINION AND AWARDS', 'PROCEDURAL AND EVIDENTIARY ISSUES', 'PROCEDURAL BACKGROUND', 'PROCEDURAL HISTORY', 'PROCEDURAL HISTORY AND POSTURE', 'PROCEDURAL HISTORY OF CLAIM', 'PROCEDURAL HISOTRY', 'PROCEDURAL MATTERS', 'PROCEDURAL MATTERS AND EVIDENTIARY RULING', 'PROCEDURAL POSTURE', 'PROCEDURAL RULING', 'PROCEDURAL RULINGS', 'RELEVANT PROCEDURAL HISTORY', 'STATEMENT OF THE CASE'],\n",
        "    \"Motions/Rulings on Motions\": ['EVIDENTIARY AND RULINGS ON MOTIONS', 'EVIDENTIARY MOTION', 'MOTION', 'MOTION FOR LEAVE TO SUBMIT ADDITIONAL EVIDENCE', 'MOTION IN LIMINE', 'MOTION RULING', 'MOTION TO ADMIT EVIDENCE', 'MOTION TO COMPEL APPROVAL OF MEDICAL TREATMENT', 'MOTION TO DISMISS', 'MOTION TO DISMISS RULING', 'MOTION TO ENLARGE TIME', 'MOTION TO EXCLUDE EXPERT TESTIMONY', 'MOTION TO ORDER SURGERY', 'MOTION TO RECONSIDER', 'MOTION TO STRIKE', 'MOTION TO SUPPLEMENT RECORD', 'MOTION TO TAKE JUDICIAL NOTICE', 'MOTIONS', 'MOTIONS AND EVIDENTIARY RULINGS', 'OUTSTANDING MOTIONS', 'PENDING MOTIONS', 'PRELIMINARY MOTIONS', 'PRETRIAL MOTIONS', 'PROCEDURAL MATTERS AND MOTIONS RULINGS', 'RULING ON GUARDIAN AD LITEM MOTION', 'RULING ON MOTION', 'RULING ON MOTION TO DISMISS', 'RULING ON MOTION TO EXCLUDE DISPUTED MEDICAL RECORDS', 'RULING ON MOTION TO STRIKE', 'RULINGS ON MOTIONS', 'RULINGS ON MOTIONS AND EVIDENTIARY MATTERS', 'SUMMARY JUDGMENT RULING', 'SUPPLEMENTAL RULING ON MOTION'],\n",
        "    \"Commissioner Details\": ['CHIEF DEPUTY COMMISSIONER', 'COMMISSIONER', 'DEPUTY COMMISSIONER', 'DEPUTY COMMISSIONER MARY CLAIRE BROWN', 'DEPUTY COMMISSIONER RONNIE ROWELL', 'DEPUTY COMMISSIONER SAM SCUDDER', 'DEPUTY COMMISSIONER_2', 'DEPUTY COMMISIONER', 'DEPUTY COMMISSISONER', 'DEPUTY OMMISSIONER', 'DEPUTYCOMMISSIONER', 'INTERIM CHIEF DEPUTY COMMISSIONER', 'OPINION AND AWARD BY CHIEF DEPUTY COMMISSIONER MELANIE WADE GOODWIN', 'OPINION AND AWARD BY DEPUTY COMMISSIONER', 'OPINION AND AWARD BY DEPUTY COMMISSIONER SAM SCUDDER', 'OPINION & AWARD BY DEPUTY COMMISSIONER TIFFANY SMITH', 'SENIOR DEPUTY COMMISSIONER'],\n",
        "    \"NC Industrial Commission Reference\": ['BEFORE THE NORTH CAROINA INDUSTRIAL COMMISSION', 'BEFORE THE NORTH CAROLINA INDUSTRIAL COMMISSION', 'INDUSTRIAL COMMISSION', 'NORTH CAROLINA INDUSTRIAL COMMISSION', 'NORTH CAROLINA INDUSTRIAL COMMISION', 'NORTHCAROLINAINDUSTRIALCOMMISSION', 'THE NORTH CAROLINA INDUSTRIAL COMMISSION'],\n",
        "    \"Consent Order/Opinion/Award\": ['AGREEMENT', 'AGREEMENT OF THE PARTIES', 'CONSENT', 'CONSENT FINDINGS OF FACT', 'CONSENT OPINION AND AWARD', 'CONSENT OPINION AND AWARD FOR PAYMENT OF DEATH BENEFITS', 'CONSENT OPINION  AWARD', 'CONSENT ORDER', 'CONSENT ORDER & AWARD', 'CONSENT ORDER AND AWARD', 'CONSENT PROPOSED OPINION AND AWARD', 'FINDINGS BY CONSENT OF THE PARTIES', 'FURTHER FINDINGS OF FACT BY CONSENT', 'ORDER AND AWARD BY CONSENT'],\n",
        "    \"Opinion and Award Title\": ['AMENDED OPINION AND AWARD', 'AMENDED OPINION & AWARD BY DEPUTY COMMISSIONER TIFFANY SMITH', 'OPINION AND AWARD', 'OPINION AND AWARD BY', 'OPINION AND AWARD FOR PAYMENT OF RATING', 'OPINION AND AWARD ON EXPEDITED MEDICAL MOTION PURSUANT TO', 'OPINION & AWARD BY DEPUTY COMMISSIONER TIFFANY SMITH', 'PROPOSED OPINION AND AWARD', 'TO THE INTERLOCUTORY OPINION AND AWARD AND AMENDED INTERLOCUTORY OPINION AND AWARD'],\n",
        "    \"Medical Related\": ['ADDITION OF VOCATIONAL REHABILITATION ISSUE BY DEFENDANT', 'ATTENDANT CARE', 'AVERAGE WEEKLY WAGE', 'CAUSAL RELATIONSHIP', 'DATE OF INJURY', 'DENTAL CONDITION', 'DISABLITY', 'INJURY BY ACCIDENT', 'MEDICAL EVIDENCE', 'MEDICAL HISTORY', 'MEDICAL RECORDS', 'MEDICAL TREATMENT', 'PERIODIC SPINE SPECIALIST EVALUATION', 'PHYSICAL INJURIES', 'PRIOR MEDICAL TREATMENT', 'PSYCHOLOGICAL', 'REIMBURSEMENT', 'SALTWATER TANK THERAPY', 'THE BED', 'THE SHOWER', 'TWICE WEEKLY MASSAGE THERAPY', 'UROLOGY REFERRAL'],\n",
        "    \"Parties/Counsel/Attorneys\": ['APPOINTMENT OF GUARDIAN AD LITEM', 'ATTORNEY FOR DEFENDANTS', 'ATTORNEY FOR EMPLOYEE', 'ATTORNEY FOR PLAINTIFF', 'COUNSEL FOR DEFENDANTS', 'COUNSEL FOR PLAINTIFF', 'DEFENDANT', 'DEFENDANT AEROTEK', 'DEFENDANT BECTON DICKINSON', 'DEFENDANT PICKETT', 'DEFENDANT PRA', 'DEFENDANT TRAVELERS', 'DEFENDANTS', 'P A R T I E S', 'PLAINTIFF', 'ROBERTS LAW FIRM', 'SCUDDER & HEDRICK', 'WARD BLACK LAW', 'YOUNG MOORE AND HENDERSON'],\n",
        "    \"Other/Miscellaneous\": ['ADDITIONAL EVIDENCE', 'ADDITIONAL EVIDENCE SUBMITTED', 'ADMINISTRATIVE ORDER', 'AMENDED', 'APPROVED', 'APPROVED WITH CONSENT OF THE PARTIES', 'CERTIFICATE OF SERVICE', 'CELESTE HARRIS', 'COMMENT', 'COMMENTARY', 'CONTENTIONS', 'CONTENTIONS OF THE PARTIES', 'COUNTY OF MECKLENBURG', 'COURT', 'CREDIBILITY', 'DOBBS BUILDING', 'DOCTOR QUESTIONNAIRE', 'EMPLOYMENT HISTORY', 'EVIDENCE', 'FURTHER RULING', 'GENERAL', 'HOLDINGS BY THE FULL COMMISSION WHICH HAVE NOT BEEN APPEALLED', 'IMOCO', 'INSURER CNA INSURANCE', 'IT APPEARING TO THE UNDERSIGNED THAT', 'IT IS HEREBY ORDERED', 'IT IS FURTHER ORDERED', 'JUDICIAL NOTICE', 'JURISDICTIONAL DISCUSSION', 'NORTH CAROLINA INSURANCE GUARANTY ASSOCIATION', 'NOTARY PUBLIC', 'NOW WITHDRAWN', 'OFFER OF PROOF', 'OFFERS OF PROOF', 'OTHER HOME MODIFICATIONS', 'OTHER ISSUES', 'OTHER MATTERS', 'OTHER RULINGS', 'OTHER STATE INSURANCE', 'PART THREE OTHER STATE INSURANCE', 'PREAMBLE', 'PREVIOUS AWARDS AND CONSENT ORDERS', 'RELEVANT TO THE INSTANT ISSUES', 'RULING', 'RULING ON JUDICIAL NOTICE', 'RULING ON SCOPE OF HEARING', 'SANCTIONS', 'SETTLEMENT TERMS', 'STATE OF NORTH CAROLINA', 'STATEMENTS', 'SUBSEQUENT TO THE CLOSING OF THE EVIDENTIARY RECORD', 'TAMMY NANCE', 'THE ALLEGED ACCIDENT AND NOTICE TO EMPLOYER', 'THOMAS VAN CAMP', 'WARD NORTH AMERICA IS DISMISSED AS A PARTY DEFENDANT AND TIG PREMIER INSURANCE COMPANY IS ADDED AS A PARTY DEFENDANT', 'WARD NORTH AMERICA PAYS THE PRO SE PLAINTIFF BENEFITS OUTSIDE THE JURISDICTION OF THE INDUSTRIAL COMMISSION', 'WITNESSETH']\n",
        "}\n",
        "\n",
        "# Create a reverse map: header variation -> standardized category name\n",
        "# Normalize by stripping whitespace and converting to uppercase for robust matching\n",
        "header_to_category_map = {}\n",
        "for category, variations in SECTION_VARIATIONS.items():\n",
        "    for variation in variations:\n",
        "        normalized_variation = variation.strip().upper()\n",
        "        if normalized_variation: # Avoid empty strings\n",
        "            header_to_category_map[normalized_variation] = category\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_dates(doc_path):\n",
        "    \"\"\"\n",
        "    Extracts beginning and end dates (Month Day, Year format) from the document.\n",
        "    Placeholder: Adapt this function based on where dates reliably appear in your docs.\n",
        "    This version scans the whole document text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        document = Document(doc_path)\n",
        "        text = '\\n'.join([para.text for para in document.paragraphs])\n",
        "\n",
        "        # Search for dates in the format \"Month Day, Year\" (e.g., \"April 23, 2014\")\n",
        "        # This regex is basic, adjust if date formats vary significantly\n",
        "        dates_found = re.findall(\n",
        "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b',\n",
        "            text\n",
        "        )\n",
        "\n",
        "        # Prioritize dates found earlier in the document if multiple exist\n",
        "        beginning_date = dates_found[0] if dates_found else 'N/A'\n",
        "        # Often the primary date is the first one. You might need more complex logic\n",
        "        # if start/end dates are explicitly labeled or always the first two.\n",
        "        # For simplicity, this example just takes the first found date.\n",
        "        # end_date = dates_found[1] if len(dates_found) > 1 else 'N/A'\n",
        "        # date_str = f\"{beginning_date} - {end_date}\" if end_date != 'N/A' else beginning_date\n",
        "\n",
        "        return beginning_date # Return only the first date found as the primary 'Date'\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not extract dates from {os.path.basename(doc_path)}: {e}\")\n",
        "        return \"N/A\"\n",
        "\n",
        "def process_word_doc_specific_sections(file_path):\n",
        "    \"\"\"\n",
        "    Reads a .docx file, identifies specific sections based on predefined header lists,\n",
        "    groups the text under standardized category names, and returns the data as a dictionary.\n",
        "    Sections end at the next recognized header or a line with '***...'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Extract Case ID from filename ---\n",
        "        base_name = os.path.basename(file_path)\n",
        "        case_id = os.path.splitext(base_name)[0] # Removes extension\n",
        "        case_id = re.sub(r'\\.(docx|doc)$', '', case_id, flags=re.IGNORECASE) # Clean potential original extension\n",
        "        # print(f\"Processing Case ID: {case_id}\") # Moved to main loop\n",
        "\n",
        "        # --- 2. Read Document ---\n",
        "        try:\n",
        "            # Add .doc conversion logic here if needed before opening\n",
        "            document = Document(file_path)\n",
        "        except Exception as e:\n",
        "            if \"File is not a zip file\" in str(e) or \"Package not found\" in str(e):\n",
        "                 print(f\"Error opening {base_name}: Invalid .docx format or corrupted. Original error: {e}\")\n",
        "            else:\n",
        "                 print(f\"Error opening document {base_name}: {e}\")\n",
        "            return None # Skip this file\n",
        "\n",
        "        # --- 3. Extract Date ---\n",
        "        date_str = extract_dates(file_path)\n",
        "\n",
        "        # --- 4. Extract Specific Sections ---\n",
        "        asterisk_delimiter_regex = re.compile(r\"^\\s*\\*{2,}\\s*$\")\n",
        "        grouped_sections = {} # Stores lists of text blocks for each category\n",
        "        current_category_key = \"Preamble\" # Default category\n",
        "        current_text_lines = []\n",
        "\n",
        "        for para in document.paragraphs:\n",
        "            para_text_stripped = para.text.strip()\n",
        "            para_text_upper = para_text_stripped.upper() # Use for matching headers\n",
        "\n",
        "            if not para_text_stripped: # Skip empty paragraphs\n",
        "                continue\n",
        "\n",
        "            recognized_category = header_to_category_map.get(para_text_upper)\n",
        "            asterisk_match = asterisk_delimiter_regex.match(para_text_stripped)\n",
        "\n",
        "            # Check if it's a recognized header\n",
        "            if recognized_category:\n",
        "                # Store previous section's text\n",
        "                if current_category_key and current_text_lines:\n",
        "                    section_text = \"\\n\".join(current_text_lines).strip()\n",
        "                    if section_text:\n",
        "                        grouped_sections.setdefault(current_category_key, []).append(section_text)\n",
        "\n",
        "                # Start new section\n",
        "                current_category_key = recognized_category\n",
        "                current_text_lines = []\n",
        "\n",
        "            # Check if it's an asterisk delimiter\n",
        "            elif asterisk_match:\n",
        "                # Store previous section's text\n",
        "                if current_category_key and current_text_lines:\n",
        "                    section_text = \"\\n\".join(current_text_lines).strip()\n",
        "                    if section_text:\n",
        "                        grouped_sections.setdefault(current_category_key, []).append(section_text)\n",
        "\n",
        "                # Stop collecting until next header\n",
        "                current_category_key = None\n",
        "                current_text_lines = []\n",
        "\n",
        "            # Otherwise, it's regular text content\n",
        "            elif current_category_key: # Only collect if we are 'inside' a section\n",
        "                current_text_lines.append(para.text) # Append original text\n",
        "\n",
        "        # Store the last section's text\n",
        "        if current_category_key and current_text_lines:\n",
        "            section_text = \"\\n\".join(current_text_lines).strip()\n",
        "            if section_text:\n",
        "                 grouped_sections.setdefault(current_category_key, []).append(section_text)\n",
        "\n",
        "        # --- 5. Prepare Final Data Dictionary for this file ---\n",
        "        final_data = {\n",
        "            \"Case ID\": case_id,\n",
        "            \"Date\": date_str,\n",
        "        }\n",
        "\n",
        "        # Join the text blocks for each category found in this document\n",
        "        for category, text_blocks in grouped_sections.items():\n",
        "            if text_blocks: # Ensure there's content\n",
        "                 valid_blocks = [block for block in text_blocks if block] # Filter empty blocks just in case\n",
        "                 if valid_blocks:\n",
        "                    # Join multiple occurrences of the same category type with double newlines\n",
        "                    final_data[category] = \"\\n\\n\".join(valid_blocks)\n",
        "\n",
        "        # Clean up potential empty preamble\n",
        "        if \"Preamble\" in final_data and not final_data[\"Preamble\"]:\n",
        "            del final_data[\"Preamble\"]\n",
        "        # Add other default fields if necessary, e.g., filename\n",
        "        # final_data[\"Source File\"] = base_name\n",
        "\n",
        "        return final_data\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # This case should ideally be handled by the main loop checking os.path.exists\n",
        "        print(f\"Error: Input file not found at {file_path}\")\n",
        "        return None\n",
        "    except ImportError:\n",
        "         # Should be caught at the start if pandas/docx aren't installed\n",
        "         print(\"Error: Required library (pandas, python-docx) not found.\")\n",
        "         raise # Re-raise to stop execution\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred processing {os.path.basename(file_path)}: {e}\")\n",
        "        # import traceback\n",
        "        # print(traceback.format_exc()) # Uncomment for detailed debugging\n",
        "        return None # Skip this file on error\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    all_extracted_data = [] # List to hold dictionaries for each file\n",
        "\n",
        "    # Check if input directory exists\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: Input directory not found: {INPUT_DIRECTORY}\")\n",
        "        print(\"Please update the INPUT_DIRECTORY variable in the script.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Starting processing in directory: {INPUT_DIRECTORY}\")\n",
        "        # List all files in the directory\n",
        "        try:\n",
        "            files_to_process = [f for f in os.listdir(INPUT_DIRECTORY)\n",
        "                                if os.path.isfile(os.path.join(INPUT_DIRECTORY, f))\n",
        "                                and f.lower().endswith('.docx')] # Process only .docx files\n",
        "        except Exception as e:\n",
        "            print(f\"Error listing files in directory {INPUT_DIRECTORY}: {e}\")\n",
        "            files_to_process = []\n",
        "\n",
        "        if not files_to_process:\n",
        "            print(\"No .docx files found in the specified directory.\")\n",
        "        else:\n",
        "            print(f\"Found {len(files_to_process)} .docx files to process.\")\n",
        "\n",
        "            # Process each file\n",
        "            for filename in files_to_process:\n",
        "                file_path = os.path.join(INPUT_DIRECTORY, filename)\n",
        "                print(f\"Processing: {filename}...\")\n",
        "                extracted_data = process_word_doc_specific_sections(file_path)\n",
        "\n",
        "                if extracted_data:\n",
        "                    all_extracted_data.append(extracted_data)\n",
        "                    print(f\"Finished: {filename}\")\n",
        "                else:\n",
        "                    print(f\"Skipped or failed: {filename}\")\n",
        "\n",
        "            # --- Create and Export Excel File ---\n",
        "            if all_extracted_data:\n",
        "                print(f\"\\nProcessed {len(all_extracted_data)} files successfully.\")\n",
        "                print(\"Creating DataFrame...\")\n",
        "\n",
        "                # Create DataFrame\n",
        "                df = pd.DataFrame(all_extracted_data)\n",
        "\n",
        "                # Define desired column order - Start with fixed columns, then add all possible section categories\n",
        "                column_order = [\"Case ID\", \"Date\"]\n",
        "                # Add all keys from SECTION_VARIATIONS to ensure all potential columns are included\n",
        "                all_section_keys = list(SECTION_VARIATIONS.keys())\n",
        "                # Add section keys only if they actually appeared in *any* document,\n",
        "                # or uncomment the line above to force all potential columns.\n",
        "                present_section_keys = [col for col in all_section_keys if col in df.columns]\n",
        "                column_order.extend(sorted(present_section_keys)) # Sort section names alphabetically\n",
        "\n",
        "                # Add any other columns that might have been generated but aren't sections (e.g., 'Preamble')\n",
        "                other_cols = sorted([col for col in df.columns if col not in column_order])\n",
        "                column_order.extend(other_cols)\n",
        "\n",
        "\n",
        "                # Reindex DataFrame to ensure consistent column order and presence\n",
        "                df = df.reindex(columns=column_order)\n",
        "\n",
        "                print(f\"Exporting data to Excel file: {OUTPUT_EXCEL_FILE}...\")\n",
        "                try:\n",
        "                    # Export to Excel\n",
        "                    df.to_excel(OUTPUT_EXCEL_FILE, index=False, engine='openpyxl')\n",
        "                    print(f\"Successfully created Excel file: {OUTPUT_EXCEL_FILE}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError writing to Excel file: {e}\")\n",
        "                    print(\"Ensure the file is not open in another application.\")\n",
        "                    print(\"You might need to install the 'openpyxl' library: pip install openpyxl\")\n",
        "\n",
        "            else:\n",
        "                print(\"\\nNo data was successfully extracted from any files.\")\n",
        "\n",
        "    print(\"\\nScript finished.\")\n"
      ],
      "metadata": {
        "id": "JQSajWQLdGFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os # Import os module for better path handling\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the path to the Excel file\n",
        "# <<< CHANGE THIS to the full path of your Excel file >>>\n",
        "excel_file_path = '/content/drive/MyDrive/worker_comp/output_extracted_data.xlsx'\n",
        "\n",
        "# Optional: Specify sheet name if not the first sheet\n",
        "# sheet_name_to_read = 'Sheet1' # Or None to read the first sheet\n",
        "\n",
        "try:\n",
        "    # Check if the file exists before attempting to read\n",
        "    if not os.path.exists(excel_file_path):\n",
        "        raise FileNotFoundError(f\"The file was not found at {excel_file_path}\")\n",
        "\n",
        "    # Read the Excel file into a pandas DataFrame\n",
        "    # If you need a specific sheet, add the sheet_name argument:\n",
        "    # df = pd.read_excel(excel_file_path, sheet_name=sheet_name_to_read)\n",
        "    df = pd.read_excel(excel_file_path) # Reads the first sheet by default\n",
        "\n",
        "    # Get the total number of rows (observations) in the DataFrame\n",
        "    total_rows = len(df)\n",
        "\n",
        "    print(f\"Analyzing Excel file: {os.path.basename(excel_file_path)}\")\n",
        "    print(f\"Total number of rows in the sheet: {total_rows}\\n\")\n",
        "    print(\"Column Analysis (Non-Empty Observations):\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Column Name':<30} | {'Non-Empty Count':<15} | {'% Non-Empty':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if total_rows > 0:\n",
        "        # Iterate through each column in the DataFrame\n",
        "        for column_name in df.columns:\n",
        "            # Get the count of non-null (non-empty) values in the column\n",
        "            # .count() in pandas excludes NaN/None values\n",
        "            non_empty_count = df[column_name].count()\n",
        "\n",
        "            # Calculate the percentage of non-empty values\n",
        "            percentage_non_empty = (non_empty_count / total_rows) * 100\n",
        "\n",
        "            # Print the results for the current column, formatted\n",
        "            print(f\"{column_name:<30} | {non_empty_count:<15} | {percentage_non_empty:>9.2f}%\")\n",
        "    else:\n",
        "        print(\"The Excel sheet is empty or contains only headers, no data rows to analyze.\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# --- Error Handling ---\n",
        "except FileNotFoundError as fnf_error:\n",
        "    print(f\"Error: {fnf_error}\")\n",
        "    print(\"Please ensure the file exists and the path is correct.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    # This error might not be directly raised by read_excel for an empty sheet,\n",
        "    # but checking total_rows handles the case of an empty DataFrame.\n",
        "    print(f\"Error: The file at {excel_file_path} seems to contain no data or only headers.\")\n",
        "except ImportError:\n",
        "     print(\"Error: The 'openpyxl' library is required to read Excel files.\")\n",
        "     print(\"Please install it using: pip install openpyxl\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading or processing the Excel file: {e}\")\n",
        "    # Consider adding more specific error handling if needed (e.g., for password-protected files)\n",
        "\n"
      ],
      "metadata": {
        "id": "uXiikKVpmrb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os # Import os module for better path handling\n",
        "\n",
        "# --- Configuration ---\n",
        "# Input file: The Excel file containing the data (e.g., the output from the previous script)\n",
        "# <<< CHANGE THIS to the full path of your INPUT Excel file >>>\n",
        "input_excel_path =  '/content/drive/MyDrive/worker_comp/output_extracted_data.xlsx'\n",
        "\n",
        "# Output file: The new Excel file with only selected columns\n",
        "# <<< CHANGE THIS (optional) to your desired OUTPUT Excel filename >>>\n",
        "output_filtered_excel_path = '/content/drive/MyDrive/worker_comp/filtered_extracted_data.xlsx'\n",
        "\n",
        "# Optional: Specify sheet name if not the first sheet in the input file\n",
        "# input_sheet_name = 'Sheet1' # Or None to read the first sheet\n",
        "\n",
        "# Columns to keep in the new file (as specified by the user)\n",
        "columns_to_keep = [\n",
        "    'Case ID',\n",
        "    'Date',\n",
        "    'Preamble',\n",
        "    'Appearances',\n",
        "    'Stipulations',\n",
        "    'Exhibits',\n",
        "    'Issues',\n",
        "    'Findings of Fact',\n",
        "    'Conclusions of Law',\n",
        "    'Order/Award'\n",
        "    # Add any other columns from the input file you wish to retain\n",
        "    # e.g., 'Source File' if it exists and you want it\n",
        "]\n",
        "\n",
        "# --- Processing ---\n",
        "try:\n",
        "    # Check if the input file exists\n",
        "    if not os.path.exists(input_excel_path):\n",
        "        raise FileNotFoundError(f\"Input file not found at {input_excel_path}\")\n",
        "\n",
        "    # Read the input Excel file\n",
        "    # Use sheet_name=input_sheet_name if reading a specific sheet\n",
        "    # df_input = pd.read_excel(input_excel_path, sheet_name=input_sheet_name)\n",
        "    df_input = pd.read_excel(input_excel_path) # Reads the first sheet by default\n",
        "    print(f\"Read {len(df_input)} rows from {os.path.basename(input_excel_path)}\")\n",
        "    print(f\"Original columns: {df_input.columns.tolist()}\")\n",
        "\n",
        "    # Check which of the desired columns actually exist in the input file\n",
        "    existing_columns_to_keep = [col for col in columns_to_keep if col in df_input.columns]\n",
        "    missing_columns = [col for col in columns_to_keep if col not in df_input.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"\\nWarning: The following requested columns were not found in the input file and will be skipped: {missing_columns}\")\n",
        "\n",
        "    if not existing_columns_to_keep:\n",
        "          print(\"\\nError: None of the requested columns were found in the input file. Cannot create the filtered file.\")\n",
        "    else:\n",
        "        # Select only the existing columns that the user wants to keep\n",
        "        df_filtered = df_input[existing_columns_to_keep]\n",
        "\n",
        "        # Write the filtered DataFrame to the new Excel file\n",
        "        # index=False prevents pandas from writing the DataFrame index as a column\n",
        "        df_filtered.to_excel(output_filtered_excel_path, index=False, engine='openpyxl')\n",
        "        print(f\"\\nSuccessfully created filtered Excel file: {output_filtered_excel_path}\")\n",
        "        print(f\"Filtered file contains columns: {existing_columns_to_keep}\")\n",
        "\n",
        "# --- Error Handling ---\n",
        "except FileNotFoundError as fnf_error:\n",
        "    print(f\"Error: {fnf_error}\")\n",
        "    print(\"Please ensure the input Excel file exists and the path is correct.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "     # This might not be directly raised by read_excel for an empty sheet,\n",
        "     # but good to keep for robustness if the file structure is unusual.\n",
        "    print(f\"Error: The input file at {input_excel_path} appears to be empty or contain no data.\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error: An unexpected issue occurred when selecting columns: {e}\")\n",
        "    print(\"This might happen if the column names in 'columns_to_keep' don't exactly match the input Excel file.\")\n",
        "except ImportError:\n",
        "     print(\"Error: The 'openpyxl' library is required to read/write Excel '.xlsx' files.\")\n",
        "     print(\"Please install it using: pip install openpyxl\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "At3xhQHInnF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}