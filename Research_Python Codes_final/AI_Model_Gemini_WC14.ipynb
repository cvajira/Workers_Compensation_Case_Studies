{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5236a1f-703d-4171-a729-4c9c14d4db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec214090-183e-4851-b829-6f16588b8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EXCEL_FILE1 =\"C:\\\\Users\\\\vmanathunga\\\\Documents\\\\Worker_comp\\\\research_ready_facts_AI.xlsx\"\n",
    "OUTPUT_EXCEL_FILE2 =\"C:\\\\Users\\\\vmanathunga\\\\Documents\\\\Worker_comp\\\\research_ready_facts_gemini_simp_gemini_2.0_flash.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645a0e1-cc8e-4ddc-a59d-34d28c385494",
   "metadata": {},
   "outputs": [],
   "source": [
    "python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b04f14-95ee-427a-a518-034cf3c13a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai google-api-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850ee47-aa26-4c98-b95e-b8e082ab91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from google.api_core import exceptions # Import for specific exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092292e-2c8c-4677-af16-484db17e87dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key =\"XXXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549f141-7ada-428a-8864-b1266aa795c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=api_key)\n",
    "# 2. Specify the Gemini Model\n",
    "# Use the latest appropriate 'flash' model identifier (e.g., gemini-1.5-flash-latest)\n",
    "# MODEL_NAME_LIST = [\"gemini-1.5-pro\", \"gemini-2.0-flash\"]\n",
    "MODEL_NAME_LIST = [\"gemini-2.0-flash\"] # Or \"gemini-1.5-flash-latest\"\n",
    "\n",
    "# 3. Excel File Paths\n",
    "INPUT_EXCEL_FILE = INPUT_EXCEL_FILE1  # Replace with your input file name\n",
    "OUTPUT_EXCEL_FILE = OUTPUT_EXCEL_FILE2 # Changed output name slightly\n",
    "\n",
    "# 4. Column Names (Adjust if different in your Excel)\n",
    "FACTS_COLUMN = \"Annonymized_Facts\"\n",
    "DECISION_COLUMN = \"AI_Decision\"\n",
    "\n",
    "# 5. Time Delays (in seconds)\n",
    "# Delay AFTER processing each row (in the main loop)\n",
    "MAIN_LOOP_DELAY_SECONDS = 0\n",
    "# Delay BEFORE each API call (inside the function) - ADDED\n",
    "API_CALL_DELAY_SECONDS = 0  # <<< Adjust this value as needed (e.g., 1, 2, 3)\n",
    "\n",
    "\n",
    "# --- Create Gemini Model Instance (ONCE) ---\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": None,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 5,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# --- Function to get decision from Gemini ---\n",
    "def get_gemini_decision(facts_text, model_instance):\n",
    "    \"\"\"\n",
    "    Sends facts to Gemini and asks for a win (1) or loss (0) decision.\n",
    "    Includes a delay before making the API call.\n",
    "\n",
    "    Args:\n",
    "        facts_text (str): The text from the 'Facts' column.\n",
    "        model_instance: The pre-configured genai.GenerativeModel instance.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 for predicted plaintiff win, 0 for predicted plaintiff loss,\n",
    "             -1 if an error occurred or decision couldn't be parsed.\n",
    "    \"\"\"\n",
    "    if not model_instance:\n",
    "        print(\"Error: Model instance is not available.\")\n",
    "        return -1\n",
    "\n",
    "    if not facts_text or not isinstance(facts_text, str) or len(facts_text.strip()) == 0:\n",
    "        print(\"Warning: Empty or invalid facts text provided.\")\n",
    "        return -1\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following legal case facts. Based solely on these facts, predict whether the plaintiff likely won or lost the case.\n",
    "\n",
    "    Respond ONLY with the number '1' if the plaintiff likely won.\n",
    "\n",
    "    Respond ONLY with the number '0' if the plaintiff likely lost.\n",
    "\n",
    "    Do NOT provide any explanation, commentary, or any text other than '1' or '0'.\n",
    "\n",
    "    Facts:\n",
    "    ---\n",
    "    {facts_text}\n",
    "    ---\n",
    "\n",
    "    Decision (1 for win, 0 for loss):\"\"\"\n",
    "\n",
    "    try:\n",
    "        # --- ADDED DELAY before API Call ---\n",
    "        print(f\"Waiting for {API_CALL_DELAY_SECONDS} second(s) before API call...\")\n",
    "        time.sleep(API_CALL_DELAY_SECONDS)\n",
    "        # ------------------------------------\n",
    "\n",
    "        # Call generate_content on the *shared* model instance\n",
    "        print(\"Making API call...\") # Added print statement\n",
    "        response = model_instance.generate_content(prompt)\n",
    "        print(\"API call complete.\") # Added print statement\n",
    "\n",
    "        # Attempt to parse the response\n",
    "        decision_text = response.text.strip()\n",
    "        if decision_text == '1':\n",
    "            return 1\n",
    "        elif decision_text == '0':\n",
    "            return 0\n",
    "        else:\n",
    "            print(f\"Warning: Could not parse decision from response: '{decision_text}' for facts: '{facts_text[:100]}...'\")\n",
    "            return -1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Gemini API call for facts: '{facts_text[:100]}...'. Error: {e}\")\n",
    "        # Check for prompt feedback in case of blocks\n",
    "        try:\n",
    "             # Check response object exists and has prompt_feedback attribute\n",
    "             if 'response' in locals() and response and hasattr(response, 'prompt_feedback') and response.prompt_feedback:\n",
    "                  print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "        except Exception as feedback_e:\n",
    "             print(f\"Could not retrieve prompt feedback: {feedback_e}\")\n",
    "        return -1\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "\n",
    "print(f\"\\nReading Excel file: {INPUT_EXCEL_FILE}\")\n",
    "try:\n",
    "    df = pd.read_excel(INPUT_EXCEL_FILE)\n",
    "    print(f\"Successfully read {len(df)} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {INPUT_EXCEL_FILE}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Check for input column\n",
    "if FACTS_COLUMN not in df.columns:\n",
    "    print(f\"Error: Column '{FACTS_COLUMN}' not found in the Excel file.\")\n",
    "    exit()\n",
    "# main function############\n",
    "\n",
    "for selected_model in MODEL_NAME_LIST:\n",
    "    print(f\"\\n===== Starting processing for Model: {selected_model} =====\")\n",
    "    try:\n",
    "      print(f\"Creating Gemini model instance: {selected_model}\")\n",
    "      shared_model_instance = genai.GenerativeModel(\n",
    "          model_name=selected_model,\n",
    "          generation_config=generation_config,\n",
    "          safety_settings=safety_settings\n",
    "          )\n",
    "      print(\"Model instance created successfully.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error creating Gemini model instance: {e}\")\n",
    "      shared_model_instance = None\n",
    "      exit()\n",
    "\n",
    "    for run_number in range(1, 2): # Loop 1 to 3 (inclusive)\n",
    "        # Create dynamic decision column name based on model and run number\n",
    "        # e.g., \"gpt-3.5-turbo-0125_AI_Decision1\", \"gpt-3.5-turbo-0125_AI_Decision2\", etc.\n",
    "        current_decision_column = f\"{selected_model}_AI_Decision{run_number}\"\n",
    "\n",
    "        print(f\"\\n--- Model: {selected_model}, Run: {run_number} ---\")\n",
    "        print(f\"Outputting decisions to column: {current_decision_column}\")\n",
    "\n",
    "        # Ensure the Decision column for this specific run/model exists, initialize if not\n",
    "        if current_decision_column not in df.columns:\n",
    "            print(f\"Initializing column: {current_decision_column}\")\n",
    "            df[current_decision_column] = -1 # Initialize with a placeholder\n",
    "        else:\n",
    "            print(f\"Column '{current_decision_column}' already exists. Values will be updated/overwritten for this run.\")\n",
    "\n",
    "        print(f\"\\nProcessing {len(df)} rows using Gemini model: {selected_model} (Run {run_number})...\")\n",
    "\n",
    "        # --- Iterate and call Gemini API for each row ---\n",
    "        total_rows = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            print(f\"\\n--- Processing row {index + 1} of {total_rows} (Model: {selected_model}, Run: {run_number}) ---\")\n",
    "            facts = str(row[FACTS_COLUMN]) if pd.notna(row[FACTS_COLUMN]) else \"\"\n",
    "\n",
    "            # Get the decision from Gemini using the shared model\n",
    "            decision = get_gemini_decision(facts, shared_model_instance)\n",
    "\n",
    "            # Update the DataFrame in the dynamically named column\n",
    "            df.loc[index, current_decision_column] = decision\n",
    "            print(f\"Row {index + 1}: Facts processed for column '{current_decision_column}'. Predicted Decision: {decision}\")\n",
    "\n",
    "            # Optional: Delay AFTER processing each row\n",
    "            if index < total_rows - 1: # Avoid sleeping after the last row of this run\n",
    "                if MAIN_LOOP_DELAY_SECONDS > 0:\n",
    "                    print(f\"Waiting for {MAIN_LOOP_DELAY_SECONDS} second(s) before next row...\")\n",
    "                    time.sleep(MAIN_LOOP_DELAY_SECONDS)\n",
    "\n",
    "        print(f\"\\n--- Finished Run {run_number} for Model: {selected_model} ---\")\n",
    "\n",
    "        # --- MODIFIED: Save the DataFrame at the end of each inner loop (run) ---\n",
    "        print(f\"Saving results to {OUTPUT_EXCEL_FILE} after Run {run_number} for Model {selected_model}...\")\n",
    "        try:\n",
    "            df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
    "            print(f\"Successfully saved results to: {OUTPUT_EXCEL_FILE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results to Excel file after Run {run_number} for Model {selected_model}: {e}\")\n",
    "        # --- END OF SAVE MODIFICATION FOR INNER LOOP ---\n",
    "\n",
    "    print(f\"\\n===== Finished all runs for Model: {selected_model} =====\")\n",
    "# --- END OF MODIFIED SECTION ---\n",
    "\n",
    "print(\"\\nAll processing complete.\") # This message now indicates all models and all their runs are done.\n",
    "\n",
    "# The final save after all processing is now removed as it's done per run.\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce78f37-8383-4b0e-83f7-308766d936e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
