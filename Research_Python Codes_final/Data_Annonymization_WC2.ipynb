{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcEkn1BivTmx"
      },
      "outputs": [],
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read the excel file with human decisions. Check for duplicate rows and drop those. Rewrite it into excel file\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fhwF8nqgwo5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_EXCEL_FILE = \"/content/drive/MyDrive/worker_comp_work/WC_Final/filtered_extracted_data.xlsx\"  # <<< CHANGE TO YOUR INPUT FILENAME"
      ],
      "metadata": {
        "id": "ExpsOVK04fwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get column names\n",
        "df = pd.read_excel(INPUT_EXCEL_FILE)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "_xELYnaH0gxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unique = df.drop_duplicates(subset=['Case ID'], keep='first')"
      ],
      "metadata": {
        "id": "OKBJ02b90-mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite to an excel file\n",
        "df_unique.to_excel(\"/content/drive/MyDrive/worker_comp_work/WC_Final/filtered_extracted_data_unique.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "xfWNS11o1Nkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get number of rows\n",
        "len(df_unique)\n",
        "#should be 15406  records"
      ],
      "metadata": {
        "id": "a9-RyBH_1d4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we plan to use only ''Case ID',''Issues', 'Findings of Fact', 'Order/Award', 'Decision' in our research.\n",
        "# choose only those columns and create a data frame\n",
        "df_research = df_unique[['Case ID', 'Issues', 'Findings of Fact', 'Order/Award', 'Decision']]\n",
        "# Create two coulmns named \"Annonymized_Facts\" and 'Annonymized_Issues' for future research\n",
        "df_research['Annonymized_Facts'] = ''\n",
        "df_research['Annonymized_Issues'] = ''\n",
        "#rewrite to an excel file for research\n",
        "df_research.to_excel(\"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "O6zgF19E1i4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_research.columns"
      ],
      "metadata": {
        "id": "IgQ7qsLHVzRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re # Import regex library for potential pattern-based additions"
      ],
      "metadata": {
        "id": "BvghCElC27pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "2VlaTWA83HWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "uDKrPhpa3MuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "MGYLvvZq3WSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re # Import regex library for potential pattern-based additions\n",
        "# --- Configuration ---\n",
        "# Select the spaCy model\n",
        "# 'en_core_web_sm' -> Small, fast, less accurate\n",
        "# 'en_core_web_md' -> Medium\n",
        "# 'en_core_web_lg' -> Large, slower, potentially more accurate\n",
        "# 'en_core_web_trf' -> Transformer-based, potentially most accurate but slowest\n",
        "NLP_MODEL_NAME = \"en_core_web_trf\"\n",
        "\n",
        "# Define the types of entities to anonymize (based on spaCy's labels)\n",
        "# Common PII labels:\n",
        "# PERSON: People, including fictional.\n",
        "# NORP: Nationalities or religious or political groups.\n",
        "# FAC: Buildings, airports, highways, bridges, etc.\n",
        "# ORG: Companies, agencies, institutions, etc.\n",
        "# GPE: Countries, cities, states.\n",
        "# LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
        "# PRODUCT: Objects, vehicles, foods, etc. (Use with caution, can be broad)\n",
        "# EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
        "# WORK_OF_ART: Titles of books, songs, etc.\n",
        "# LAW: Named documents made into laws.\n",
        "# DATE: Absolute or relative dates or periods.\n",
        "# TIME: Times smaller than a day.\n",
        "# MONEY: Monetary values, including unit.\n",
        "# QUANTITY: Measurements, as of weight or distance.\n",
        "# ORDINAL: \"first\", \"second\", etc.\n",
        "# CARDINAL: Numerals that do not fall under another type.\n",
        "\n",
        "# **Choose carefully based on your needs!** Start specific.\n",
        "PII_LABELS_TO_ANONYMIZE = {\n",
        "    \"PERSON\",\n",
        "    \"NORP\",\n",
        "    \"FAC\",\n",
        "    \"GPE\",      # Cities, States, Countries\n",
        "    \"LOC\",      # Other locations (mountains, rivers)\n",
        "    \"ORG\",      # Organizations, companies\n",
        "    \"EVENT\",\n",
        "    \"WORK_OF_ART\",\n",
        "    \"LAW\",\n",
        "    \"DATE\",\n",
        "    \"TIME\",\n",
        "    \"MONEY\",\n",
        "    \"QUANTITY\",\n",
        "    #\"ORDINAL\",\n",
        "    \"CARDINAL\",\n",
        "    \"PHONE\",    # Custom label for regex\n",
        "    \"EMAIL\",    # Custom label for regex\n",
        "    # Add more as needed, e.g., \"FAC\", \"NORP\", \"MONEY\"\n",
        "}\n",
        "\n",
        "# Define custom regex patterns for things spaCy might miss\n",
        "# (Simple examples, enhance as needed)\n",
        "REGEX_PATTERNS = {\n",
        "    \"EMAIL\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
        "    \"PHONE\": r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\" # Simple US-like phone format\n",
        "    # Add patterns for IDs, addresses etc. if needed\n",
        "}\n",
        "\n",
        "# --- Load spaCy Model ---\n",
        "try:\n",
        "    nlp = spacy.load(NLP_MODEL_NAME)\n",
        "    print(f\"Loaded spaCy model '{NLP_MODEL_NAME}'\")\n",
        "except OSError:\n",
        "    print(f\"spaCy model '{NLP_MODEL_NAME}' not found.\")\n",
        "    print(f\"Please run: python -m spacy download {NLP_MODEL_NAME}\")\n",
        "    nlp = None # Set nlp to None if loading fails\n",
        "\n",
        "# --- Anonymization Function ---\n",
        "def anonymize_text(text):\n",
        "    if not nlp:\n",
        "        print(\"spaCy model not loaded. Cannot perform NER-based anonymization.\")\n",
        "        return text # Or raise an error\n",
        "\n",
        "    # --- Step 1: Apply Regex Replacements First ---\n",
        "    # Create a list of replacements to apply later to avoid modifying\n",
        "    # the string while iterating with regex. Store (start, end, placeholder).\n",
        "    regex_replacements = []\n",
        "    for label, pattern in REGEX_PATTERNS.items():\n",
        "        if label in PII_LABELS_TO_ANONYMIZE:\n",
        "            for match in re.finditer(pattern, text):\n",
        "                regex_replacements.append((match.start(), match.end(), f\"[{label}]\"))\n",
        "\n",
        "    # Sort regex replacements by start index\n",
        "    regex_replacements.sort()\n",
        "\n",
        "    # Apply regex replacements, adjusting for length changes if needed\n",
        "    # (Simpler approach: build a new string like in spaCy part)\n",
        "    processed_text = \"\"\n",
        "    last_end = 0\n",
        "    for start, end, placeholder in regex_replacements:\n",
        "        # Avoid overlapping replacements if necessary (basic check)\n",
        "        if start >= last_end:\n",
        "            processed_text += text[last_end:start]\n",
        "            processed_text += placeholder\n",
        "            last_end = end\n",
        "    processed_text += text[last_end:]\n",
        "    text = processed_text # Update text for spaCy processing\n",
        "\n",
        "\n",
        "    # --- Step 2: Apply spaCy NER Replacements ---\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Store identified entities to replace (start_char, end_char, label)\n",
        "    entities_to_replace = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in PII_LABELS_TO_ANONYMIZE:\n",
        "            entities_to_replace.append((ent.start_char, ent.end_char, f\"[{ent.label_}]\"))\n",
        "\n",
        "    # Sort entities by start position to process the text sequentially\n",
        "    entities_to_replace.sort()\n",
        "\n",
        "    # Build the anonymized string piece by piece\n",
        "    anonymized_text = \"\"\n",
        "    current_pos = 0\n",
        "    for start, end, placeholder in entities_to_replace:\n",
        "        # Add the text segment before the current entity\n",
        "        # Important: Check for overlaps caused by regex or nested entities\n",
        "        if start >= current_pos:\n",
        "             anonymized_text += text[current_pos:start]\n",
        "             # Add the placeholder\n",
        "             anonymized_text += placeholder\n",
        "             # Update the current position to the end of the replaced entity\n",
        "             current_pos = end\n",
        "        # If overlap (start < current_pos), means this entity was already part\n",
        "        # of a replaced segment (e.g., regex replaced an email containing a name)\n",
        "        # or nested entity. We skip adding it again.\n",
        "\n",
        "    # Add any remaining text after the last entity\n",
        "    anonymized_text += text[current_pos:]\n",
        "\n",
        "    return anonymized_text"
      ],
      "metadata": {
        "id": "fRlpKwCav42X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Annonymizing Finding of Fact Column\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Excel File & Column Configuration ---\n",
        "INPUT_EXCEL_FILE = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready.xlsx\"\n",
        "OUTPUT_EXCEL_FILE = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready2.xlsx\"\n",
        "FACTS_COLUMN_NAME = \"Findings of Fact\"\n",
        "ANONYMIZED_COLUMN_NAME = \"Annonymized_Facts\"\n",
        "\n",
        "# --- Main Processing Logic ---\n",
        "if 'nlp' in locals():  # Check if nlp is defined\n",
        "    print(f\"\\nReading Excel file: {INPUT_EXCEL_FILE}\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_EXCEL_FILE)\n",
        "        print(f\"Successfully read {len(df)} rows.\")\n",
        "\n",
        "        # Check if the input column exists\n",
        "        if FACTS_COLUMN_NAME not in df.columns:\n",
        "            print(f\"Error: Column '{FACTS_COLUMN_NAME}' not found in the Excel file.\")\n",
        "            exit()\n",
        "\n",
        "        # Apply the anonymization function to the 'Facts' column\n",
        "        print(f\"Anonymizing text in column '{FACTS_COLUMN_NAME}'...\")\n",
        "\n",
        "        def anonymize_and_print_row(row):\n",
        "            if pd.notna(row[FACTS_COLUMN_NAME]):\n",
        "                print(f\"Anonymizing row: {row.name}\")\n",
        "                # Assuming anonymize_text is defined elsewhere\n",
        "                return anonymize_text(str(row[FACTS_COLUMN_NAME]))\n",
        "            else:\n",
        "                return \"\"\n",
        "\n",
        "        df[ANONYMIZED_COLUMN_NAME] = df.apply(anonymize_and_print_row, axis=1)\n",
        "\n",
        "        print(\"Anonymization complete.\")\n",
        "\n",
        "        # Save the updated DataFrame to a new Excel file\n",
        "        print(f\"Saving results to: {OUTPUT_EXCEL_FILE}\")\n",
        "        try:\n",
        "            df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
        "            print(\"Successfully saved anonymized data.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results to Excel file: {e}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at '{INPUT_EXCEL_FILE}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Excel processing: {e}\")\n",
        "else:\n",
        "    print(\"\\nCannot proceed with Excel processing because spaCy model ('nlp') is not loaded or defined.\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "id": "8IpWQwJxXnCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Annonymizing Issues column\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Excel File & Column Configuration ---\n",
        "INPUT_EXCEL_FILE = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready2.xlsx\"\n",
        "OUTPUT_EXCEL_FILE = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready3.xlsx\"\n",
        "FACTS_COLUMN_NAME = \"Issues\"\n",
        "ANONYMIZED_COLUMN_NAME = \"Annonymized_Issues\"\n",
        "\n",
        "# --- Main Processing Logic ---\n",
        "if 'nlp' in locals():  # Check if nlp is defined\n",
        "    print(f\"\\nReading Excel file: {INPUT_EXCEL_FILE}\")\n",
        "    try:\n",
        "        df = pd.read_excel(INPUT_EXCEL_FILE)\n",
        "        print(f\"Successfully read {len(df)} rows.\")\n",
        "\n",
        "        # Check if the input column exists\n",
        "        if FACTS_COLUMN_NAME not in df.columns:\n",
        "            print(f\"Error: Column '{FACTS_COLUMN_NAME}' not found in the Excel file.\")\n",
        "            exit()\n",
        "\n",
        "        # Apply the anonymization function to the 'Facts' column\n",
        "        print(f\"Anonymizing text in column '{FACTS_COLUMN_NAME}'...\")\n",
        "\n",
        "        def anonymize_and_print_row(row):\n",
        "            if pd.notna(row[FACTS_COLUMN_NAME]):\n",
        "                print(f\"Anonymizing row: {row.name}\")\n",
        "                # Assuming anonymize_text is defined elsewhere\n",
        "                return anonymize_text(str(row[FACTS_COLUMN_NAME]))\n",
        "            else:\n",
        "                return \"\"\n",
        "\n",
        "        df[ANONYMIZED_COLUMN_NAME] = df.apply(anonymize_and_print_row, axis=1)\n",
        "\n",
        "        print(\"Anonymization complete.\")\n",
        "\n",
        "        # Save the updated DataFrame to a new Excel file\n",
        "        print(f\"Saving results to: {OUTPUT_EXCEL_FILE}\")\n",
        "        try:\n",
        "            df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
        "            print(\"Successfully saved anonymized data.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results to Excel file: {e}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at '{INPUT_EXCEL_FILE}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Excel processing: {e}\")\n",
        "else:\n",
        "    print(\"\\nCannot proceed with Excel processing because spaCy model ('nlp') is not loaded or defined.\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "id": "pL5rtT1CoTxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}