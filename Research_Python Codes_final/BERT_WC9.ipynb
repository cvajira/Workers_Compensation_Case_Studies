{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Using BERT embedding approach to classify Decision according to Issues"
      ],
      "metadata": {
        "id": "_ZPHSejP4Oig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ETiatDyM5EBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas torch transformers datasets tokenizers scikit-learn openpyxl"
      ],
      "metadata": {
        "id": "s5v9PhYW__aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "metadata": {
        "id": "451EddMf9uEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiesZb2X34mC"
      },
      "outputs": [],
      "source": [
        "INPUT_EXCEL_FILE1 = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_Issues_train.xlsx\"\n",
        "INPUT_EXCEL_FILE2 = \"/content/drive/MyDrive/worker_comp_work/WC_Final/research_ready_Issues_test.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load training and testing files for Issues\n",
        "df_train_issues = pd.read_excel(INPUT_EXCEL_FILE1)\n",
        "df_test_issues = pd.read_excel(INPUT_EXCEL_FILE2)"
      ],
      "metadata": {
        "id": "PN1iWITW5R2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_issues.columns"
      ],
      "metadata": {
        "id": "H8DoQILP5dRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_issues.shape\n",
        "#should be 4882"
      ],
      "metadata": {
        "id": "CJtQMX_-5iuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_issues.shape\n",
        "#should be 1221"
      ],
      "metadata": {
        "id": "LCU7SfHg5m9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check unique values in 'Decision' column\n",
        "df_train_issues['Decision'].unique()"
      ],
      "metadata": {
        "id": "X8gi7Oyg5tAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the distribution of Decsion variable in train and test as percentage\n",
        "df_train_issues['Decision'].value_counts(normalize=True)\n",
        "#should be about 63.25%-36.75%"
      ],
      "metadata": {
        "id": "62b08Ayj52jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_issues['Decision'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "Kp9G-OQW7Sfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_t = df_train_issues['Preprocesses_Issues'].tolist()\n",
        "test_text = df_test_issues['Preprocesses_Issues'].tolist()\n",
        "\n",
        "train_l = df_train_issues['Decision'].tolist()\n",
        "test_labels = df_test_issues['Decision'].tolist()"
      ],
      "metadata": {
        "id": "A3L1AEca7eq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ApS1z6nANM2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LENGTH = 256  # Max sequence length for BERT\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Split training data again into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_t, train_l, test_size=0.2, random_state=42, stratify=train_l\n",
        ")\n",
        "\n",
        "# 2. Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_data(texts, labels):\n",
        "    \"\"\"Tokenizes text data and prepares inputs for BERT.\"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            text,\n",
        "                            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "                            max_length=MAX_LENGTH,\n",
        "                            pad_to_max_length=True,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt',     # Return PyTorch tensors\n",
        "                            truncation=True\n",
        "                       )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "train_input_ids, train_attention_masks, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
        "val_input_ids, val_attention_masks, val_labels_tensor = tokenize_data(val_texts, val_labels)\n",
        "\n",
        "# 3. Create PyTorch Datasets and DataLoaders\n",
        "class IssueDataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for BERT.\"\"\"\n",
        "    def __init__(self, input_ids, attention_masks, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = IssueDataset(train_input_ids, train_attention_masks, train_labels_tensor)\n",
        "val_dataset = IssueDataset(val_input_ids, val_attention_masks, val_labels_tensor)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 4. Define and Initialize BERT Model\n",
        "# BertForSequenceClassification has a linear layer on top for classification.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,  # Binary classification (0 or 1)\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "model.to(device) # Move model to GPU if available\n",
        "\n",
        "# 5. Optimizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0, # Default value\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# 6. Training Loop\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids,\n",
        "                        token_type_ids=None, # Not needed for BERT sequence classification\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits # Raw scores, before SoftMax\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward() # Backward pass to calculate gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients to prevent exploding gradients\n",
        "        optimizer.step() # Update weights\n",
        "        scheduler.step() # Update learning rate\n",
        "\n",
        "    avg_train_loss = total_loss / len(data_loader)\n",
        "    return avg_train_loss\n",
        "\n",
        "# 7. Evaluation Function\n",
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"Evaluates the model on the validation set.\"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU for sklearn\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "            predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "            actual_labels.extend(label_ids.flatten())\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(actual_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(actual_labels, predictions, average='binary', zero_division=0)\n",
        "\n",
        "    return avg_val_loss, accuracy, precision, recall, f1\n",
        "\n",
        "# --- Training and Evaluation ---\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
        "\n",
        "    avg_train_loss = train_epoch(model, train_dataloader, optimizer, device, scheduler)\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    avg_val_loss, accuracy, precision, recall, f1 = eval_model(model, val_dataloader, device)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# --- (Optional) Save the model and tokenizer ---\n",
        "# output_dir = './bert_decision_classifier/'\n",
        "# import os\n",
        "# if not os.path.exists(output_dir):\n",
        "#     os.makedirs(output_dir)\n",
        "# print(f\"Saving model to {output_dir}\")\n",
        "# model.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# --- (Optional) How to make predictions on new data ---\n",
        "def predict(texts_to_predict, model, tokenizer, device, max_length=MAX_LENGTH):\n",
        "    \"\"\"Makes predictions on a list of new texts.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    for text in texts_to_predict:\n",
        "        encoded_review = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_review['input_ids'].to(device)\n",
        "        attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        prediction = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "        all_predictions.append(prediction)\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "# Example of predicting new issues:\n",
        "# new_issues = [\n",
        "#     \"The website is down again.\",\n",
        "#     \"Everything seems to be working fine today.\",\n",
        "#     \"Need help with password reset.\"\n",
        "# ]\n",
        "# predictions = predict(new_issues, model, tokenizer, device)\n",
        "# for issue, pred in zip(new_issues, predictions):\n",
        "#     print(f\"Issue: '{issue}' --> Predicted Decision: {pred}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UKxofkw1NIRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(test_text, model, tokenizer, device)"
      ],
      "metadata": {
        "id": "LiR-HFuAOf-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a code for calculating model performance metrics and deliver it as one single data frame\n",
        "def model_metrics(pred, targets):\n",
        "  #pred=model.predict(predictors)\n",
        " # pred_prob=model.predict_proba(predictors)[:,1]\n",
        "\n",
        "  acc=accuracy_score(targets, pred)\n",
        "  rec=recall_score(targets, pred)\n",
        "  spec=specificity_score(targets, pred, average='binary')\n",
        "  prec=precision_score(targets, pred)\n",
        "  f1=f1_score(targets, pred)\n",
        "  auc=roc_auc_score(targets,pred)\n",
        "\n",
        "  df_metrics=pd.DataFrame({}, index=['Metrics'])\n",
        "  df_metrics['Accuracy']=acc\n",
        "  df_metrics['Recall']=rec\n",
        "  df_metrics['Specificity']=spec\n",
        "  df_metrics['Precision']=prec\n",
        "  df_metrics['F1']=f1\n",
        "  df_metrics['AUC']=auc\n",
        "\n",
        "  return df_metrics"
      ],
      "metadata": {
        "id": "6GjFQJ24I5_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "5qR5e_moKHb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and Display confusion matrix\n",
        "from imblearn.metrics import specificity_score\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "def display_confusion_matrix(pred, targets):\n",
        "  #pred=model.predict(predictors)\n",
        "  cm=confusion_matrix(targets, pred)\n",
        "  #cm_percentage=cm.astype('float')/cm.sum()*100\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "  disp.plot(cmap=plt.cm.Blues)\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kLF5xplZIzb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_metrics(predictions, test_labels)"
      ],
      "metadata": {
        "id": "lW4S3XQbPpEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_confusion_matrix(predictions, test_labels)"
      ],
      "metadata": {
        "id": "cmjNkGR9Pv3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}